---
title: "Critical Thinking Group 4 - HW1"
author: "Sreejaya, Suman, Vuthy"
date: "September 12, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(RCurl)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(gridExtra)
library(psych)
library(reshape)
library(MASS)
library(car)
library(recommenderlab)
library(knitr)
```

#Purpose  

The purpose of this experiment is to try to predict the amount of wins for a baseball team using the (modified) moneyball dataset. This dataset contains approximately 2200 observations with 17 variables. Each observation represents the performance of a professional baseball team from 1871 to 2006. The statistics have been adjusted to match the performance of a 162 game season.  

Dataset:  
[Moneyball Training Data](https://github.com/Nguyver/DATA621-HW/blob/master/HW1/moneyball-training-data.csv)  
[Moneyball Evaluation Data](https://github.com/Nguyver/DATA621-HW/blob/master/HW1/moneyball-evaluation-data.csv)

#1. Data Exploration

```{r}
#read directly from the github
moneyballTraining <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW1/moneyball-training-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

The dependent (response) variable is *TARGET_WINS*. Excluding INDEX, the rest of the variables are the independent variables (predictors). Lets review how each of these independent variables are distributed & how each of these indepdent variable relates to the response variable 'TARGET_WINS'.


##1.1 Missing Values
Review the *measure of the center* for the given variables. A quick look at the summary statistics indicate that there are missing values for some of the predictors.

```{r, warning=FALSE}
summary(moneyballTraining[3:17])
```

The list of predictor variables with missing data and their counts:

```{r, warning=FALSE, message=FALSE}
moneyball.NA <- apply(moneyballTraining[3:17], 2, function(x) sum(is.na(x)))
moneyball.missing <- cbind(moneyball.NA, moneyball.NA/nrow(moneyballTraining))
colnames(moneyball.missing) <- c('Missing', 'Percentage')
kable(moneyball.missing)
```

##1.2 Distribution of predictors
Review the distributions of the predictors. Here are few histograms of the predictors.

```{r, warning=FALSE, message=FALSE}
# Explore independent variable TEAM_BATTING_H
g_tbh <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_H), binwidth = 0.5) + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_b2b <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_2B), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_brsb <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BASERUN_SB), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tph <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_PITCHING_H), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tps <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_PITCHING_SO), binwidth = 0.5) + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tfe <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_FIELDING_E), binwidth = 0.5) + theme(axis.text=element_text(size=8),axis.title=element_text(size=8))

g_tfd <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_FIELDING_DP), binwidth = 0.5) + theme(axis.text=element_text(size=8),axis.title=element_text(size=8))

g_tbhr <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_HR), binwidth = 0.5) + theme(axis.text=element_text(size=8),axis.title=element_text(size=8))

g_tphLg <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=log(TEAM_PITCHING_H)), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tpsLg <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=log(TEAM_PITCHING_SO)), binwidth = 0.5) + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

grid.arrange(g_tbh, g_b2b,g_brsb, g_tph, g_tps, g_tfe,g_tfd,g_tbhr,g_tphLg,g_tpsLg, ncol=2)
```

Based on the summary of the data, and the histograms, there are outliers and the distributions of the few of the predictors are skewed. Notice that *TEAM_PITCHING_H* and *TEAM_PICTCHING_SO* distributions are not visible at all in the above diagram, so the log transformation has been applied in the above.

Lets also review the box plot's of the predictors.

```{r, warning=FALSE, message=FALSE}
meltMoneyBallTraining <- melt(moneyballTraining[3:17])
ggplot(meltMoneyBallTraining, aes(factor(variable), value)) + geom_boxplot() + facet_wrap(~variable, scale="free")  + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```

##1.3 Standard Deviation

```{r}
getStandardDev <- function(moneyballTraining)
{
  stdDevs <- SD(moneyballTraining[3:17])
  par(mai=c(3,1.2,1,1))

  # transformed the y, due to high variances.
  barplot(stdDevs[order(stdDevs, decreasing = T)], log = "y", las=2, main="Std Dev of Predictors", xlab="", ylab="Log(SD)", cex.axis = 0.8, cex.names=0.8) 
  
  return(stdDevs)
}

std <- getStandardDev(moneyballTraining)
kable(as.data.frame(std))
```

##1.4 Correlation
Find correlation of Response variable with predictor variables
```{r}

corData <-  round(cor(moneyballTraining), 3)                    # rounding makes it easier to look at
t.corData <- t(corData[2,c(2:17)])   # we are only interested on correlation of Team win against all other predictors
moneyballTraining.cor <- melt(t.corData) # convert the wide format to long form for ease of read
moneyballTraining.cor <- moneyballTraining.cor[, 2:3]
colnames(moneyballTraining.cor) <- c('Variable', 'Correlation')

kable(moneyballTraining.cor)
## TEAM_BATTING_SO,TEAM_PITCHING_H,  TEAM_FIELDING_E  have negative correlation with total win. (which is expected)

#TEAM_PITCHING_SO,TEAM_FIELDING_DP (but this should be positive)

## TEAM_BASERUN_CS, TEAM_BATTING_HBP have very low correlation with Total win. So we dont have to consider these variables in the MODEL
```
  
From the above *TEAM_BATTING_H* is high positively correlated, and *TEAM_FIELDING_E* is lower side of negative corelation with the *TARGET_WINS*

```{r, warning=FALSE, message=FALSE}
g1 = ggplot(data = moneyballTraining) + geom_point(aes(x=TEAM_BATTING_H, y= TARGET_WINS), alpha = 0.2, color="blue") + ggtitle("TARGET WINS  Vs TEAM_BATTING_H") 

g2 = ggplot(data = moneyballTraining) + geom_point(aes(x=TEAM_FIELDING_E, y= TARGET_WINS), alpha = 0.2, color="red") + ggtitle("TARGET WINS  Vs TEAM_FIELDING_E") 
 
grid.arrange(g1, g2, nrow=2)
#similarly other specific independent variables Vs target wins correlation diagram
```

#2. Data Preparation

##2.1 Imputation of issing data

We have noticed that there are missing values for predictors, lets impute of missing values with mean.

```{r}
#Replacing Missing Values In dataset with column mean
for(i in 1:ncol(moneyballTraining)){
  moneyballTraining[is.na(moneyballTraining[,i]), i] <- mean(moneyballTraining[,i], na.rm = TRUE)
}
```

After imputation, the missing values should not be there.

```{r}
mb.imp <- apply(moneyballTraining[3:17], 2, function(x) sum(is.na(x)))
#colnames(mb.imp) <- c('# Missing')
kable(as.data.frame(mb.imp))
```

Correlation of response variable to predictor variable after imputing data
```{r}
corData.imp <-  round(cor(moneyballTraining), 3)                    # rounding makes it easier to look at
t.corData.imp <- t(corData.imp[2,c(2:17)])   # we are only interested on correlation of Team win against all other predictors
moneyballTraining.cor.imp <- melt(t.corData.imp) # convert the wide format to long form for ease of read
moneyballTraining.cor.imp <- moneyballTraining.cor.imp[, 2:3]

colnames(moneyballTraining.cor.imp) <- c('Variable', 'Correlation')
kable(moneyballTraining.cor.imp)
```


#3. Build  Models
To fit a multiple linear regression model with TARGET_WINS as the response variable all the other predictors as the explanatory variables except 'TEAM BASERUN CS' &  'TEAM BATTING HBP' as they have very low correlation with Wins:
We eliminate TEAM_BASERUN_CS

## 3.1 Manual elimination
```{r}
#Full Model

team.mod1 <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_BATTING_HBP +
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)
summary(team.mod1)

 #R-squared:  0.3192,  Adjusted R-squared:  0.3147
team.mod2 <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + 
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)

summary(team.mod2)
#R-squared:  0.3188,  Adjusted R-squared:  0.3149  
#Adjusted R-squared is higher than the original one.  

```

##3.2 Stepwise Regression
```{r}
fit <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_BATTING_HBP + 
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)

# Stepwise Regression
step <- stepAIC(fit, direction="both")
step$anova # display results
summary(step)
#R-squared:  0.3186,  Adjusted R-squared:  0.3153 

par(mfrow=c(2, 2))
plot(step, main="Stepwise Regression")
```
####Model with Stepwise Regression
TARGET_WINS = 23.67 + 0.048*TEAM_BATTING_H + -0.020*TEAM_BATTING_2B + 0.0625*TEAM_BATTING_3B + 
    0.0698*TEAM_BATTING_HR + 0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 0.029*TEAM_BASERUN_SB + 
    -0.0007*TEAM_PITCHING_H + 0.0029*TEAM_PITCHING_SO + -0.0206*TEAM_FIELDING_E + -0.121*TEAM_FIELDING_DP
    
##3.3 Backward elimination model   
```{r}
step.backward <- step(fit, direction="backward")
step$anova # display results
summary(step.backward)

#R-squared:  0.3186,  Adjusted R-squared:  0.3153
par(mfrow=c(2, 2))
plot(step.backward, main="Backward elimination")
```
###Model with Backward elimination
TARGET_WINS = 23.66 + 0.048*TEAM_BATTING_H + -0.020*TEAM_BATTING_2B + 
    0.062*TEAM_BATTING_3B + 0.0698*TEAM_BATTING_HR + 0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 
    0.029*TEAM_BASERUN_SB + -0.001*TEAM_PITCHING_H + 0.002*TEAM_PITCHING_SO + -0.021*TEAM_FIELDING_E + 
    -0.121*TEAM_FIELDING_DP

##3.4 forward Elimination
```{r}
forward.null <-lm(TARGET_WINS~1,data=moneyballTraining)
forward.full <-lm(TARGET_WINS~.,data=moneyballTraining)
step.forward<- step(forward.null, scope=list(lower=forward.null, upper=forward.full), direction="forward")

summary(step.forward)
# R-squared:  0.3188,  Adjusted R-squared:  0.3152
par(mfrow=c(2, 2))
plot(step.forward, main="Forward Elimination")
```
###Model with forward elimination
TARGET_WINS = 24.23 + 0.048*TEAM_BATTING_H + -0.021*TEAM_FIELDING_E + 
    0.029*TEAM_BASERUN_SB + -0.12*TEAM_FIELDING_DP + 0.015*TEAM_PITCHING_HR + 0.061*TEAM_BATTING_3B + 
    0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 0.003*TEAM_PITCHING_SO + -0.0007*TEAM_PITCHING_H + 
    0.054*TEAM_BATTING_HR + -0.02*TEAM_BATTING_2B


##Backward elimination and Stepwise Regression Model has the best Adjusted R-squared value.
###So we would like select Backward elimination model  



##Adding the below simple regression analysis, we can remove if it does not make sense in final version. 

##3.5 Regression Analysis by removing high Variance Inflation Factor(VIF), and high p value predictors
```{r}
glimpse(moneyballTraining)

#Lets first break the training set into training and test sets.
set.seed(11)
samples <- sample(1:nrow(moneyballTraining), 0.75*nrow(moneyballTraining))
moneyballTraining <- moneyballTraining[samples,]
moneyballTest <- moneyballTraining[-samples,]
glimpse(moneyballTraining)
glimpse(moneyballTest)

#Let's consider ALL the variables ( except INDEX).
fit1 <- lm(TARGET_WINS ~ . -INDEX, data=moneyballTraining)
summary(fit1)

#Lets check for Multi-Collinearity - lets find vif value and drop those that has 
#got high vif (>5)
vifFit1 <- vif(fit1)
vifFit1

#sort by descending
sort(vifFit1, decreasing = T)

#These has got high vif value [ highly correlated results in multicolinearity ]
#TEAM_BATTING_HR TEAM_PITCHING_HR  TEAM_BATTING_BB TEAM_PITCHING_BB 
#36.901942        29.744427         6.815457         6.363007

fit2 <- lm(TARGET_WINS ~ .-INDEX -TEAM_BATTING_HR -TEAM_PITCHING_HR -TEAM_BATTING_BB -TEAM_PITCHING_BB, data=moneyballTraining)

#Lets now review the summary of this model, and look at the p-values now. Lets get rid of the
#variables with p-value > 0.05
summary(fit2)

# These are all got p value > 0.05, lets drop these in our next model
#TEAM_BATTING_3B   3.188e-02  1.879e-02   1.697  0.089971 
#TEAM_BATTING_SO   5.331e-05  2.521e-03   0.021  0.983129
#TEAM_BATTING_HBP  1.027e-01  8.655e-02   1.187  0.235449  
#TEAM_BATTING_2B  -1.584e-02  1.063e-02  -1.490  0.136465 

fit3 <- lm(TARGET_WINS ~ .-INDEX -TEAM_BATTING_HR -TEAM_PITCHING_HR -TEAM_BATTING_BB -TEAM_PITCHING_BB -TEAM_BATTING_3B, data=moneyballTraining)

summary(fit3)

#TEAM_BATTING_SO  -0.0016982  0.0023009  -0.738 0.4606 
#TEAM_BATTING_HBP  0.1004744  0.0865918   1.160 0.2461
#TEAM_PITCHING_H  -0.0004844  0.0004140  -1.170 0.2422
#TEAM_BATTING_2B  -0.0174378  0.0105980  -1.645 0.1001

#Lets remove TEAM_BATTING_SO, TEAM_BATTING_HBP,TEAM_PITCHING_H,TEAM_BATTING_2B
fit4 <- lm(TARGET_WINS ~ .-INDEX -TEAM_BATTING_HR -TEAM_PITCHING_HR -TEAM_BATTING_BB -TEAM_PITCHING_BB -TEAM_BATTING_3B -TEAM_BATTING_SO -TEAM_BATTING_HBP -TEAM_PITCHING_H -TEAM_BATTING_2B, data=moneyballTraining)

summary(fit4)

```

###3.5.1. RMSE - Root Mean Sqared Error (verification with test data)
```{r}
#Lets take our model, and apply it on the test dataset.
predicted.wins <- predict(fit4, newdata = moneyballTest)

#Lets calculate the RMSE
residuals <- moneyballTest$TARGET_WINS - predicted.wins
(rmse_test <- sqrt(mean(residuals^2)))
#high rmse [13.3875]

#lets put in ggplot
rmse.df <- data.frame(actual = moneyballTest$TARGET_WINS, predicted = predicted.wins)

#dev.off()
ggplot(rmse.df, aes(x=actual, y = predicted)) + geom_point()

#hmmm. there appears to be few outliers
```

###3.5.2 Diagnostic plots, check for linearity, normality is justified for residuals ...
```{r}
#Check if linearity in residuals violated
plot(fit4, which=1)

#Observation: the red line is about flat, which indicates the linearity in residuals is good.


#Is residual variance is constant [ assumption of homo scedasticity is fine or not ]
plot(fit4, which=3)


#For normality of residuals
plot(fit4, which=2) #if most of the resids are on the straight line we are good to go.

#Lets check normality via ggplot for specific data element
# say, TEAM_BATTING_HR
d = data.frame(x=fit4$residuals, y = moneyballTraining$TEAM_BATTING_HR)
ggplot(d, aes(x,y)) + geom_point() + geom_smooth()

ggplot(d, aes(x)) + geom_histogram()

ggplot(d, aes(x)) + geom_density(color="red") + stat_function(fun=dnorm, args=list(mean=mean(d$x), sd=sd(d$x)), color="dark green") #residuals are normal.
```

# 4. Selection
```{r}
moneyballEvaluation <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW1/moneyball-evaluation-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

#check how many na's for each column
apply(moneyballEvaluation, 2, function(x) sum(is.na(x)))

#Replacing Missing Values In dataset with column mean
for(i in 1:ncol(moneyballEvaluation)){
  moneyballEvaluation[is.na(moneyballEvaluation[,i]), i] <- mean(moneyballEvaluation[,i], na.rm = TRUE)
}

apply(moneyballEvaluation, 2, function(x) sum(is.na(x)))

eval.wins <- predict(fit4, newdata = moneyballEvaluation)

head(eval.wins)
```

# A. Appendix

```{r eval=FALSE, echo=TRUE, options(width = 80)}

library(RCurl)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(gridExtra)
library(psych)
library(reshape)
library(MASS)
library(car)
library(recommenderlab)
library(knitr)
#opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)

moneyballTraining <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW1/moneyball-training-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

summary(moneyballTraining[3:17])

moneyball.NA <- apply(moneyballTraining[3:17], 2, function(x) sum(is.na(x)))
moneyball.missing <- cbind(moneyball.NA, moneyball.NA/nrow(moneyballTraining))
colnames(moneyball.missing) <- c('Missing', 'Percentage')
kable(moneyball.missing)

# Explore independent variable TEAM_BATTING_H
g_tbh <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_H), binwidth = 0.5) + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_b2b <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_2B), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_brsb <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BASERUN_SB), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tph <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_PITCHING_H), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tps <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_PITCHING_SO), binwidth = 0.5) + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tfe <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_FIELDING_E), binwidth = 0.5) + theme(axis.text=element_text(size=8),axis.title=element_text(size=8))

g_tfd <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_FIELDING_DP), binwidth = 0.5) + theme(axis.text=element_text(size=8),axis.title=element_text(size=8))

g_tbhr <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_HR), binwidth = 0.5) + theme(axis.text=element_text(size=8),axis.title=element_text(size=8))

g_tphLg <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=log(TEAM_PITCHING_H)), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tpsLg <- ggplot(data = moneyballTraining) + geom_histogram(aes(x=log(TEAM_PITCHING_SO)), binwidth = 0.5) + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

grid.arrange(g_tbh, g_b2b,g_brsb, g_tph, g_tps, g_tfe,g_tfd,g_tbhr,g_tphLg,g_tpsLg, ncol=2)

meltMoneyBallTraining <- melt(moneyballTraining[3:17])
ggplot(meltMoneyBallTraining, aes(factor(variable), value)) + geom_boxplot() + facet_wrap(~variable, scale="free")  + theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

getStandardDev <- function(moneyballTraining)
{
  stdDevs <- SD(moneyballTraining[3:17])
  par(mai=c(3,1.2,1,1))
  
  # transformed the y, due to high variances.
  barplot(stdDevs[order(stdDevs, decreasing = T)], log = "y", las=2, main="Std Dev of Predictors", xlab="", ylab="Log(SD)", cex.axis = 0.8, cex.names=0.8) 
  
  return(stdDevs)
}

std <- getStandardDev(moneyballTraining)
kable(as.data.frame(std))

corData <-  round(cor(moneyballTraining), 3)                    # rounding makes it easier to look at
t.corData <- t(corData[2,c(2:17)])   # we are only interested on correlation of Team win against all other predictors
moneyballTraining.cor <- melt(t.corData) # convert the wide format to long form for ease of read
moneyballTraining.cor <- moneyballTraining.cor[, 2:3]
colnames(moneyballTraining.cor) <- c('Variable', 'Correlation')

kable(moneyballTraining.cor)

g1 = ggplot(data = moneyballTraining) + geom_point(aes(x=TEAM_BATTING_H, y= TARGET_WINS), alpha = 0.2, color="blue") + ggtitle("TARGET WINS  Vs TEAM_BATTING_H") 

g2 = ggplot(data = moneyballTraining) + geom_point(aes(x=TEAM_FIELDING_E, y= TARGET_WINS), alpha = 0.2, color="red") + ggtitle("TARGET WINS  Vs TEAM_FIELDING_E") 

grid.arrange(g1, g2, nrow=2)
#similarly other specific independent variables Vs target wins correlation diagram

#Replacing Missing Values In dataset with column mean
for(i in 1:ncol(moneyballTraining)){
  moneyballTraining[is.na(moneyballTraining[,i]), i] <- mean(moneyballTraining[,i], na.rm = TRUE)
}

mb.imp <- apply(moneyballTraining[3:17], 2, function(x) sum(is.na(x)))
#colnames(mb.imp) <- c('# Missing')
kable(as.data.frame(mb.imp))

corData.imp <-  round(cor(moneyballTraining), 3)                    # rounding makes it easier to look at
t.corData.imp <- t(corData.imp[2,c(2:17)])   # we are only interested on correlation of Team win against all other predictors
moneyballTraining.cor.imp <- melt(t.corData.imp) # convert the wide format to long form for ease of read
moneyballTraining.cor.imp <- moneyballTraining.cor.imp[, 2:3]

colnames(moneyballTraining.cor.imp) <- c('Variable', 'Correlation')
kable(moneyballTraining.cor.imp)
```

