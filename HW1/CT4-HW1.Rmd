---
title: "CT4 - HW1"
author: "Sreejaya, Suman, Vuthy"
date: "September 12, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
suppressMessages(library(RCurl))
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))
suppressMessages(library(psych))
```

#Critical Thinking Group 4 - HW1

## Data Exploration

Describe the size and the variables in the moneyball training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you aren't doing your job. Some suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment. You should have your own thoughts on what to tell the boss. These are just ideas.  

a. Mean / Standard Deviation / Median
b. Bar Chart or Box Plot of the data
c. Is the data correlated to the target variable (or to other variables?)
d. Are any of the variables missing and need to be imputed "fixed"?


```{r echo=FALSE}
#read directly from the github
moneyballTraining <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW1/moneyball-training-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

There are a few variables that are missing data.  

- TEAM_BATTING_SO (Strikeouts by batters): 102
- TEAM_BASERUN_SB (Stolen bases): 131
- TEAM_BASERUN_CS (Caught stealing): 772
- TEAM_BATTING_HBP (Batters hit by pitch): 2085
- TEAM_PITCHING_SO (Strikeouts by pitchers): 102
- TEAM_FIELDING_DP (Double Plays): 286

```{r echo=FALSE, eval=FALSE}
head(moneyballTraining)
names(moneyballTraining)
```

```{r}
summary(moneyballTraining)
```

Some of the data might have outliers. There are a few histograms with long tails.
```{r}
hist(moneyballTraining$TEAM_BASERUN_SB, main="Histogram of Stolen bases", xlab = "Stolen bases")
hist(moneyballTraining$TEAM_FIELDING_E, main="Histogram of Errors", xlab="Errors")
hist(moneyballTraining$TEAM_PITCHING_BB, main="Histogram of Walks allowed", xlab="Walks allowed")
hist(moneyballTraining$TEAM_PITCHING_H, main="Histogram of Hits allowed", xlab="Hits allowed")
hist(moneyballTraining$TEAM_PITCHING_SO, main="Strikeouts by pitchers", xlab="Strikeouts by pitchers")
```

```{r}
# 'TARGET_WINS' is the Dependent/Response variable here. And below are the 
# independent (/exploratory) variables.
# Indpendent Variables: TEAM_BATTING_H TEAM_BATTING_2B TEAM_BATTING_3B  
# TEAM_BATTING_HR  TEAM_BATTING_BB TEAM_BATTING_SO  TEAM_BASERUN_SB TEAM_BASERUN_CS

# Let us try to explore each of the exploratory variable's distribution 
# [histogram, boxplot - to understand the distribution and identify outliers etc.]
# Let us also explore how each of the independent variable related to the 
# response variable ( using scatter plot )
# what else ?


# Explore independent variable TEAM_BATTING_H
ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_H), binwidth = 0.5) + 
  ggtitle(paste("Histogram  - " , "TEAM_BATTING_H")) 

qplot(y=moneyballTraining$TEAM_BATTING_H, x= 1, geom = "boxplot")

ggplot(data = moneyballTraining) + 
  geom_point(aes(x=TEAM_BATTING_H, y= TARGET_WINS), alpha = 0.2, color="blue") + 
  ggtitle("TARGET WINS  Vs TEAM_BATTING_H") 
```

Hey Suman, Can you verify this section is correct. There seems to be some merging issues. I know you cleaned up the merge but looks like it might have been reintroduced. I had to comment out some stuff but check to make sure I didn't remove stuff you wanted to keep

Summary to review data distribution, NA's. Impute of missing values and view Standard Deviation before and after imputation.
```{r}

summary(moneyballTraining[3:17])

dev.off()

#Visualize the Standard deviations in all the independent variables -Suman.
#--------------------------------------------------------------------------
getStandardDev <- function(moneyballTraining)
{
  stdDevs <- SD(moneyballTraining[3:17])
  stdDevs[order(stdDevs, decreasing = T)]
  
  par(mai=c(3,1.2,1,1))
  
  # transformed the y, due to high variances.
  barplot(stdDevs[order(stdDevs, decreasing = T)], log = "y", las=2, 
          main="Std Dev of Predictors", xlab="", ylab="Log(SD)", 
          cex.axis = 0.8, cex.names=0.8)
  
  mtext("Predictors", side=1, line = 8.5, las=1)
  return(stdDevs)
}
# =======
# 
# Summary to review data distribution, NA's. Impute of missing values and view Standard Deviation before and after imputation.
# 
# ```{r}
# 
# summary(moneyballTraining[3:17])
# 
# dev.off()
# 
# #Visualize the Standard deviations in all the independent variables -Suman.
# #--------------------------------------------------------------------------
# getStandardDev <- function(moneyballTraining)
# {
#   stdDevs <- SD(moneyballTraining[3:17])
#   stdDevs[order(stdDevs, decreasing = T)]
#   
#   par(mai=c(3,1.2,1,1))
#   
#   # transformed the y, due to high variances.
#   barplot(stdDevs[order(stdDevs, decreasing = T)], log = "y", las=2, main="Std Dev of Predictors", xlab="", ylab="Log(SD)", cex.axis = 0.8, cex.names=0.8)
#   +mtext("Predictors", side=1, line = 8.5, las=1)
#   return(stdDev)
# }
# 
# 
# >>>>>>> origin/master
getStandardDev(moneyballTraining)
#--------------------------------------------------------------------------

#Replacing Missing Values In dataset with column mean
for(i in 1:ncol(moneyballTraining)){
  moneyballTraining[is.na(moneyballTraining[,i]), i] <- mean(moneyballTraining[,i], na.rm = TRUE)
}

#After imputation , verify the standard deviation again.
getStandardDev(moneyballTraining)
```

Finding Outliers using boxplot
```{r}
#Box plot to find outliers
par(mfrow=c(3,5))
boxplot(moneyballTraining$TEAM_BATTING_H, main="TEAM_BATTING_H")
boxplot(moneyballTraining$TEAM_BATTING_2B, main="TEAM_BATTING_2B")
boxplot(moneyballTraining$TEAM_BATTING_3B, main="TEAM_BATTING_3B")
boxplot(moneyballTraining$TEAM_BATTING_HR, main="TEAM_BATTING_HR")
boxplot(moneyballTraining$TEAM_BATTING_BB, main="TEAM_BATTING_BB")

boxplot(moneyballTraining$TEAM_BATTING_SO, main="TEAM_BATTING_SO")
boxplot(moneyballTraining$TEAM_BASERUN_SB, main="TEAM_BASERUN_SB")
boxplot(moneyballTraining$TEAM_BASERUN_CS, main="TEAM_BASERUN_CS")
boxplot(moneyballTraining$TEAM_BATTING_HBP, main="TEAM_BATTING_HBP")
boxplot(moneyballTraining$TEAM_PITCHING_H, main="TEAM_PITCHING_H")

boxplot(moneyballTraining$TEAM_PITCHING_HR, main="TEAM_PITCHING_HR")
boxplot(moneyballTraining$TEAM_PITCHING_BB, main="TEAM_PITCHING_BB")
boxplot(moneyballTraining$TEAM_PITCHING_SO, main="TEAM_PITCHING_SO")
boxplot(moneyballTraining$TEAM_FIELDING_E, main="TEAM_FIELDING_E")
boxplot(moneyballTraining$TEAM_FIELDING_DP, main="TEAM_FIELDING_DP")

par(mfrow=c(3, 5))
for (response in c("TEAM_BATTING_H" ,  "TEAM_BATTING_2B",  "TEAM_BATTING_3B" ,
  "TEAM_BATTING_HR" , "TEAM_BATTING_BB",  "TEAM_BATTING_SO",  "TEAM_BASERUN_SB" , "TEAM_BASERUN_CS", 
 "TEAM_BATTING_HBP","TEAM_PITCHING_H",  "TEAM_PITCHING_HR", "TEAM_PITCHING_BB", "TEAM_PITCHING_SO",
"TEAM_FIELDING_E" , "TEAM_FIELDING_DP"))
 boxplot(moneyballTraining[, response] ~ TARGET_WINS, data=moneyballTraining, ylab=response)
dev.off()
```

## DATA PREPARATION

Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.  

a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables

```{r}
# Do we imput the missing data? Or drop records
```

Removing Outliers
```{r}
#
# comparing median to mean. Usually median and mean should be the same. But if median is far lower than the mean it could suggest that there are outliers.

#from the BoxPlot we see outliers for 

#TEAM_BATTING_3B, TEAM_BASERUN_SB,TEAM_BASERUN_CS, TEAM_PITCHING_H, TEAM_PITCHING_BB, TEAM_PITCHING_#SO, TEAM_FIELDING_E
#since TEAM_BASERUN_CS and TEAM_PITCHING_SO have same mean and median we can assume that they dont have any outliers.

##?? do we need to remove zero's? Max values seems reasonable or not??
```

find correlation of Response variable with predictor variables
```{r}

corData <-  round(cor(moneyballTraining), 3)                    # rounding makes it easier to look at
library(reshape)
t(corData[2,c(2:17)])   # we are only interested on correlation of Team win against all other predictors
moneyballTraining.cor <- melt(t(corData[2,c(2:17)])) # convert the wide format to long form for ease of read
moneyballTraining.cor

## TEAM_BATTING_SO,TEAM_PITCHING_H,  TEAM_FIELDING_E  have negative correlation with total win. (which is expected)

#TEAM_PITCHING_SO,TEAM_FIELDING_DP (but this should be positive)

## TEAM_BASERUN_CS, TEAM_BATTING_HBP have very low correlation with Total win. So we dont have to consider these variables in the MODEL
```

Predictor Variables  Vs Response variable(Target win) 
Correlation Visualization
```{r}
 
#plt_TEAM_BATTING_H<-ggplot(data = moneyballTraining,aes(x=TEAM_BATTING_H,y=TARGET_WINS)) + geom_point() + #geom_smooth(method=glm)  + ggtitle(paste("Plot  - " , "TEAM_BATTING_H")) 

plt_TEAM_BATTING_H <- ggplot(moneyballTraining, aes(TEAM_BATTING_H,TARGET_WINS))
plt_TEAM_BATTING_H <-plt_TEAM_BATTING_H + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_H"))

plt_TEAM_BATTING_2B <- ggplot(moneyballTraining, aes(TEAM_BATTING_2B,TARGET_WINS))
plt_TEAM_BATTING_2B <-plt_TEAM_BATTING_2B + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_2B"))

plt_TEAM_BATTING_3B <- ggplot(moneyballTraining, aes(TEAM_BATTING_3B,TARGET_WINS))
plt_TEAM_BATTING_3B <-plt_TEAM_BATTING_3B + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_3B"))

plt_TEAM_BATTING_HR <- ggplot(moneyballTraining, aes(TEAM_BATTING_HR,TARGET_WINS))
plt_TEAM_BATTING_HR <-plt_TEAM_BATTING_HR + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_HR"))

plt_TEAM_BATTING_BB <- ggplot(moneyballTraining, aes(TEAM_BATTING_BB,TARGET_WINS))
plt_TEAM_BATTING_BB <-plt_TEAM_BATTING_BB + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_BB"))

plt_TEAM_BATTING_SO <- ggplot(moneyballTraining, aes(TEAM_BATTING_SO,TARGET_WINS))
plt_TEAM_BATTING_SO <-plt_TEAM_BATTING_SO + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_SO"))

plt_TEAM_BASERUN_SB <- ggplot(moneyballTraining, aes(TEAM_BASERUN_SB,TARGET_WINS))
plt_TEAM_BASERUN_SB <-plt_TEAM_BASERUN_SB + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BASERUN_SB"))


plt_TEAM_BASERUN_CS <- ggplot(moneyballTraining, aes(TEAM_BASERUN_CS,TARGET_WINS))
plt_TEAM_BASERUN_CS <-plt_TEAM_BASERUN_CS + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BASERUN_CS"))

plt_TEAM_BATTING_HBP <- ggplot(moneyballTraining, aes(TEAM_BATTING_HBP,TARGET_WINS))
plt_TEAM_BATTING_HBP <-plt_TEAM_BATTING_HBP + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_HBP"))

plt_TEAM_PITCHING_H <- ggplot(moneyballTraining, aes(TEAM_PITCHING_H,TARGET_WINS))
plt_TEAM_PITCHING_H <-plt_TEAM_PITCHING_H + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_H"))

plt_TEAM_PITCHING_HR <- ggplot(moneyballTraining, aes(TEAM_PITCHING_HR,TARGET_WINS))
plt_TEAM_PITCHING_HR <-plt_TEAM_PITCHING_HR + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_HR"))

plt_TEAM_PITCHING_BB <- ggplot(moneyballTraining, aes(TEAM_PITCHING_BB,TARGET_WINS))
plt_TEAM_PITCHING_BB <-plt_TEAM_PITCHING_BB + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_BB"))

plt_TEAM_PITCHING_SO <- ggplot(moneyballTraining, aes(TEAM_PITCHING_SO,TARGET_WINS))
plt_TEAM_PITCHING_SO <-plt_TEAM_PITCHING_SO + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_SO"))

plt_TEAM_FIELDING_E <- ggplot(moneyballTraining, aes(TEAM_FIELDING_E,TARGET_WINS))
plt_TEAM_FIELDING_E <-plt_TEAM_FIELDING_E + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_FIELDING_E"))

plt_TEAM_FIELDING_DP <- ggplot(moneyballTraining, aes(TEAM_FIELDING_DP,TARGET_WINS))
plt_TEAM_FIELDING_DP <-plt_TEAM_FIELDING_DP + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_FIELDING_DP"))

#All Predictor Variables
grid.arrange(plt_TEAM_BATTING_H, plt_TEAM_BATTING_2B, plt_TEAM_BATTING_3B, plt_TEAM_BATTING_HR, plt_TEAM_BATTING_BB,plt_TEAM_BATTING_SO, plt_TEAM_BASERUN_SB, plt_TEAM_BASERUN_CS, plt_TEAM_BATTING_HBP, plt_TEAM_PITCHING_H, plt_TEAM_PITCHING_HR, plt_TEAM_PITCHING_SO, plt_TEAM_FIELDING_E, plt_TEAM_FIELDING_DP, plt_TEAM_PITCHING_BB,    ncol = 3, nrow = 5)

#positive impact on wins

grid.arrange(plt_TEAM_BATTING_H, plt_TEAM_BATTING_2B, plt_TEAM_BATTING_3B, plt_TEAM_BATTING_HR, plt_TEAM_BATTING_BB, plt_TEAM_BASERUN_SB,  plt_TEAM_BATTING_HBP,   plt_TEAM_PITCHING_SO,  plt_TEAM_FIELDING_DP,   ncol = 3, nrow = 5)

#negative impact on wins
grid.arrange(plt_TEAM_BATTING_SO,  plt_TEAM_BASERUN_CS,  plt_TEAM_PITCHING_H, plt_TEAM_PITCHING_HR,  plt_TEAM_FIELDING_E,  plt_TEAM_PITCHING_BB,    ncol = 3, nrow = 5)
```

## BUILD MODELS

Using the training data set, build at least three different multiple linear regression models, using different variables (or the same variables with different transformations). Since we have not yet covered automated variable selection methods, you should select the variables manually (unless you previously learned Forward or Stepwise selection, etc.). Since you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.  

Discuss the coefficients in the models, do they make sense? For example, if a team hits a lot of Home Runs, it would be reasonably expected that such a team would win more games. However, if the coefficient is negative (suggesting that the team would lose more games), then that needs to be discussed. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.

## 3.Build  Models
To fit a multiple linear regression model with TARGET_WINS as the response variable all the other predictors as the explanatory variables except 'TEAM BASERUN CS' &  'TEAM BATTING HBP' as they have very low correlation with Wins:
We eliminate TEAM_BASERUN_CS

# 3.1 Manual elimination
```{r}
#Full Model

team.mod1 <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_BATTING_HBP +
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)
summary(team.mod1)

 #R-squared:  0.3192,  Adjusted R-squared:  0.3147
team.mod2 <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + 
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)

summary(team.mod2)
#R-squared:  0.3188,  Adjusted R-squared:  0.3149  
#Adjusted R-squared is higher than the original one.  


```

#3.2 Stepwise Regression
```{r}
library(MASS)
fit <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_BATTING_HBP + 
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)

# Stepwise Regression
step <- stepAIC(fit, direction="both")
step$anova # display results
summary(step)
#R-squared:  0.3186,  Adjusted R-squared:  0.3153 

par(mfrow=c(2, 2))
plot(step,, main="Stepwise Regression")
```
#Model with Stepwise Regression
TARGET_WINS = 23.67 + 0.048*TEAM_BATTING_H + -0.020*TEAM_BATTING_2B + 0.0625*TEAM_BATTING_3B + 
    0.0698*TEAM_BATTING_HR + 0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 0.029*TEAM_BASERUN_SB + 
    -0.0007*TEAM_PITCHING_H + 0.0029*TEAM_PITCHING_SO + -0.0206*TEAM_FIELDING_E + -0.121*TEAM_FIELDING_DP
    
#3.3 Backward elimination model   
```{r}
step.backward <- step(fit, direction="backward")
step$anova # display results
summary(step.backward)

#R-squared:  0.3186,  Adjusted R-squared:  0.3153
par(mfrow=c(2, 2))
plot(step.backward, main="Backward elimination")
```
#Model with Backward elimination
TARGET_WINS = 23.66 + 0.048*TEAM_BATTING_H + -0.020*TEAM_BATTING_2B + 
    0.062*TEAM_BATTING_3B + 0.0698*TEAM_BATTING_HR + 0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 
    0.029*TEAM_BASERUN_SB + -0.001*TEAM_PITCHING_H + 0.002*TEAM_PITCHING_SO + -0.021*TEAM_FIELDING_E + 
    -0.121*TEAM_FIELDING_DP

#3.4 forward Elimination
```{r}
forward.null <-lm(TARGET_WINS~1,data=moneyballTraining)
forward.full <-lm(TARGET_WINS~.,data=moneyballTraining)
step.forward<- step(forward.null, scope=list(lower=forward.null, upper=forward.full), direction="forward")

summary(step.forward)
# R-squared:  0.3188,  Adjusted R-squared:  0.3152
par(mfrow=c(2, 2))
plot(step.forward, main="Forward Elimination")
```
#Model with forward elimination
TARGET_WINS = 24.23 + 0.048*TEAM_BATTING_H + -0.021*TEAM_FIELDING_E + 
    0.029*TEAM_BASERUN_SB + -0.12*TEAM_FIELDING_DP + 0.015*TEAM_PITCHING_HR + 0.061*TEAM_BATTING_3B + 
    0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 0.003*TEAM_PITCHING_SO + -0.0007*TEAM_PITCHING_H + 
    0.054*TEAM_BATTING_HR + -0.02*TEAM_BATTING_2B


##Backward elimination and Stepwise Regression Model has the best Adjusted R-squared value.
###So we would like select Backward elimination model  



## SELECT MODELS

Decide on the criteria for selecting the best multiple linear regression model. Will you select a model with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your model.  

For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data set.

```{r}

```
