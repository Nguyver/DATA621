---
title: "CT4 - HW1"
author: "Sreejaya, Suman, Vuthy"
date: "September 12, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Critical Thinking Group 4 - HW1

Let me know what you guys think. We can use this file to do the coding and document details for each other. At the end, when we are done, we put together a much cleaner document without all the details and code we left for each other.  


```{r}
## load libraries ####

library(RCurl)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(psych)
library(reshape)
library(lattice)
library(car)
library(recommenderlab)
```

Read in the file to do Exploratory Data Analysis

```{r}
#read directly from the github
moneyballTraining <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW1/moneyball-training-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

head(moneyballTraining)
names(moneyballTraining)
summary(moneyballTraining)

#Cleaning ?


#Data Exploration:

# 'TARGET_WINS' is the Dependent/Response variable here. And below are the independent (/exploratory) variables.
# Indpendent Variables: TEAM_BATTING_H TEAM_BATTING_2B TEAM_BATTING_3B  TEAM_BATTING_HR  TEAM_BATTING_BB TEAM_BATTING_SO  TEAM_BASERUN_SB TEAM_BASERUN_CS

# Let us try to explore each of the exploratory variable's distribution [histogram, boxplot - to understand the distribution and identify outliers etc.]
# Let us also explore how each of the independent variable related to the response variable ( using scatter plot )
# what else ?

#
library(ggplot2)

# Explore independent variable TEAM_BATTING_H
ggplot(data = moneyballTraining) + geom_histogram(aes(x=TEAM_BATTING_H), binwidth = 0.5) + ggtitle(paste("Histogram  - " , "TEAM_BATTING_H")) 

qplot(y=moneyballTraining$TEAM_BATTING_H, x= 1, geom = "boxplot")

ggplot(data = moneyballTraining) + geom_point(aes(x=TEAM_BATTING_H, y= TARGET_WINS), alpha = 0.2, color="blue") + ggtitle("TARGET WINS  Vs TEAM_BATTING_H") 

```


sreejaya
```{r}
##Load Data

moneyballtrain.URL <- getURL("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW1/moneyball-training-data.csv")
moneyballTraining <- read.csv(text = moneyballtrain.URL,header = TRUE, stringsAsFactors = FALSE)
names(moneyballTraining)



```

Summary to review data distribution, NA's. Impute of missing values and view Standard Deviation before and after imputation.
```{r}

summary(moneyballTraining[3:17])

dev.off()

#Visualize the Standard deviations in all the independent variables -Suman.
#--------------------------------------------------------------------------
getStandardDev <- function(moneyballTraining)
{
  stdDevs <- SD(moneyballTraining[3:17])
  stdDevs[order(stdDevs, decreasing = T)]
  
  par(mai=c(3,1.2,1,1))
  
  # transformed the y, due to high variances.
  barplot(stdDevs[order(stdDevs, decreasing = T)], log = "y", las=2, main="Std Dev of Predictors", xlab="", ylab="Log(SD)", cex.axis = 0.8, cex.names=0.8)
  +mtext("Predictors", side=1, line = 8.5, las=1)
  return(stdDev)
}


summary(moneyballTraining[3:17])

dev.off()

#Visualize the Standard deviations in all the independent variables -Suman.
#--------------------------------------------------------------------------
getStandardDev <- function(moneyballTraining)
{
  stdDevs <- SD(moneyballTraining[3:17])
  stdDevs[order(stdDevs, decreasing = T)]
  
  par(mai=c(3,1.2,1,1))
  
  # transformed the y, due to high variances.
  barplot(stdDevs[order(stdDevs, decreasing = T)], log = "y", las=2, main="Std Dev of Predictors", xlab="", ylab="Log(SD)", cex.axis = 0.8, cex.names=0.8)
  +mtext("Predictors", side=1, line = 8.5, las=1)
  return(stdDev)
}


getStandardDev(moneyballTraining)


#Replacing Missing Values In dataset with column mean
for(i in 1:ncol(moneyballTraining)){
  moneyballTraining[is.na(moneyballTraining[,i]), i] <- mean(moneyballTraining[,i], na.rm = TRUE)
}

#After imputation , verify the standard deviation again.
getStandardDev(moneyballTraining)
```

Finding Outliers using boxplot
```{r}
#Box plot to find outliers
par(mfrow=c(3,5))
boxplot(moneyballTraining$TEAM_BATTING_H, main="TEAM_BATTING_H")
boxplot(moneyballTraining$TEAM_BATTING_2B, main="TEAM_BATTING_2B")
boxplot(moneyballTraining$TEAM_BATTING_3B, main="TEAM_BATTING_3B")
boxplot(moneyballTraining$TEAM_BATTING_HR, main="TEAM_BATTING_HR")
boxplot(moneyballTraining$TEAM_BATTING_BB, main="TEAM_BATTING_BB")

boxplot(moneyballTraining$TEAM_BATTING_SO, main="TEAM_BATTING_SO")
boxplot(moneyballTraining$TEAM_BASERUN_SB, main="TEAM_BASERUN_SB")
boxplot(moneyballTraining$TEAM_BASERUN_CS, main="TEAM_BASERUN_CS")
boxplot(moneyballTraining$TEAM_BATTING_HBP, main="TEAM_BATTING_HBP")
boxplot(moneyballTraining$TEAM_PITCHING_H, main="TEAM_PITCHING_H")

boxplot(moneyballTraining$TEAM_PITCHING_HR, main="TEAM_PITCHING_HR")
boxplot(moneyballTraining$TEAM_PITCHING_BB, main="TEAM_PITCHING_BB")
boxplot(moneyballTraining$TEAM_PITCHING_SO, main="TEAM_PITCHING_SO")
boxplot(moneyballTraining$TEAM_FIELDING_E, main="TEAM_FIELDING_E")
boxplot(moneyballTraining$TEAM_FIELDING_DP, main="TEAM_FIELDING_DP")

par(mfrow=c(3, 5))
for (response in c("TEAM_BATTING_H" ,  "TEAM_BATTING_2B",  "TEAM_BATTING_3B" ,
  "TEAM_BATTING_HR" , "TEAM_BATTING_BB",  "TEAM_BATTING_SO",  "TEAM_BASERUN_SB" , "TEAM_BASERUN_CS", 
 "TEAM_BATTING_HBP","TEAM_PITCHING_H",  "TEAM_PITCHING_HR", "TEAM_PITCHING_BB", "TEAM_PITCHING_SO",
"TEAM_FIELDING_E" , "TEAM_FIELDING_DP"))
+ boxplot(moneyballTraining[, response] ~ TARGET_WINS, data=moneyballTraining, ylab=response)
dev.off()
```


Removing Outliers
```{r}
#
# comparing median to mean. Usually median and mean should be the same. But if median is far lower than the mean it could suggest that there are outliers.

#from the BoxPlot we see otliers for 

#TEAM_BATTING_3B, TEAM_BASERUN_SB,TEAM_BASERUN_CS, TEAM_PITCHING_H, TEAM_PITCHING_BB, TEAM_PITCHING_#SO, TEAM_FIELDING_E
# since TEAM_BASERUN_CS and TEAM_PITCHING_SO have same mean and median we can assume that they dont have ant outliers.

##?? do we need to remove zero's? Max values seems reasonable or not??
```

find correlation of Response variable with predictor variables
```{r}

corData <-  round(cor(moneyballTraining), 3)                    # rounding makes it easier to look at
library(reshape)
t(corData[2,c(2:17)])   # we are only interested on correlation of Team win against all other predictors
moneyballTraining.cor <- melt(t(corData[2,c(2:17)])) # convert the wide format to long form for ease of read
moneyballTraining.cor

## TEAM_BATTING_SO,TEAM_PITCHING_H,  TEAM_FIELDING_E  have negative correlation with total win. (which is expected)

#TEAM_PITCHING_SO,TEAM_FIELDING_DP (but this should be positive)

## TEAM_BASERUN_CS, TEAM_BATTING_HBP have very low correlation with Total win. So we dont have to consider these variables in the MODEL


```

Predictor Variables  Vs Response variable(Target win) 
Correlation Visualization
```{r}
 
#plt_TEAM_BATTING_H<-ggplot(data = moneyballTraining,aes(x=TEAM_BATTING_H,y=TARGET_WINS)) + geom_point() + #geom_smooth(method=glm)  + ggtitle(paste("Plot  - " , "TEAM_BATTING_H")) 

plt_TEAM_BATTING_H <- ggplot(moneyballTraining, aes(TEAM_BATTING_H,TARGET_WINS))
plt_TEAM_BATTING_H <-plt_TEAM_BATTING_H + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_H"))

plt_TEAM_BATTING_2B <- ggplot(moneyballTraining, aes(TEAM_BATTING_2B,TARGET_WINS))
plt_TEAM_BATTING_2B <-plt_TEAM_BATTING_2B + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_2B"))

plt_TEAM_BATTING_3B <- ggplot(moneyballTraining, aes(TEAM_BATTING_3B,TARGET_WINS))
plt_TEAM_BATTING_3B <-plt_TEAM_BATTING_3B + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_3B"))

plt_TEAM_BATTING_HR <- ggplot(moneyballTraining, aes(TEAM_BATTING_HR,TARGET_WINS))
plt_TEAM_BATTING_HR <-plt_TEAM_BATTING_HR + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_HR"))

plt_TEAM_BATTING_BB <- ggplot(moneyballTraining, aes(TEAM_BATTING_BB,TARGET_WINS))
plt_TEAM_BATTING_BB <-plt_TEAM_BATTING_BB + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_BB"))

plt_TEAM_BATTING_SO <- ggplot(moneyballTraining, aes(TEAM_BATTING_SO,TARGET_WINS))
plt_TEAM_BATTING_SO <-plt_TEAM_BATTING_SO + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_SO"))

plt_TEAM_BASERUN_SB <- ggplot(moneyballTraining, aes(TEAM_BASERUN_SB,TARGET_WINS))
plt_TEAM_BASERUN_SB <-plt_TEAM_BASERUN_SB + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BASERUN_SB"))


plt_TEAM_BASERUN_CS <- ggplot(moneyballTraining, aes(TEAM_BASERUN_CS,TARGET_WINS))
plt_TEAM_BASERUN_CS <-plt_TEAM_BASERUN_CS + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BASERUN_CS"))

plt_TEAM_BATTING_HBP <- ggplot(moneyballTraining, aes(TEAM_BATTING_HBP,TARGET_WINS))
plt_TEAM_BATTING_HBP <-plt_TEAM_BATTING_HBP + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_BATTING_HBP"))

plt_TEAM_PITCHING_H <- ggplot(moneyballTraining, aes(TEAM_PITCHING_H,TARGET_WINS))
plt_TEAM_PITCHING_H <-plt_TEAM_PITCHING_H + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_H"))

plt_TEAM_PITCHING_HR <- ggplot(moneyballTraining, aes(TEAM_PITCHING_HR,TARGET_WINS))
plt_TEAM_PITCHING_HR <-plt_TEAM_PITCHING_HR + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_HR"))

plt_TEAM_PITCHING_BB <- ggplot(moneyballTraining, aes(TEAM_PITCHING_BB,TARGET_WINS))
plt_TEAM_PITCHING_BB <-plt_TEAM_PITCHING_BB + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_BB"))

plt_TEAM_PITCHING_SO <- ggplot(moneyballTraining, aes(TEAM_PITCHING_SO,TARGET_WINS))
plt_TEAM_PITCHING_SO <-plt_TEAM_PITCHING_SO + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_PITCHING_SO"))

plt_TEAM_FIELDING_E <- ggplot(moneyballTraining, aes(TEAM_FIELDING_E,TARGET_WINS))
plt_TEAM_FIELDING_E <-plt_TEAM_FIELDING_E + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_FIELDING_E"))

plt_TEAM_FIELDING_DP <- ggplot(moneyballTraining, aes(TEAM_FIELDING_DP,TARGET_WINS))
plt_TEAM_FIELDING_DP <-plt_TEAM_FIELDING_DP + stat_smooth(method='gam') + geom_point()+ ggtitle(paste("Plot  - " , "TEAM_FIELDING_DP"))

#All Predictor Variables
grid.arrange(plt_TEAM_BATTING_H, plt_TEAM_BATTING_2B, plt_TEAM_BATTING_3B, plt_TEAM_BATTING_HR, plt_TEAM_BATTING_BB,plt_TEAM_BATTING_SO, plt_TEAM_BASERUN_SB, plt_TEAM_BASERUN_CS, plt_TEAM_BATTING_HBP, plt_TEAM_PITCHING_H, plt_TEAM_PITCHING_HR, plt_TEAM_PITCHING_SO, plt_TEAM_FIELDING_E, plt_TEAM_FIELDING_DP, plt_TEAM_PITCHING_BB,    ncol = 3, nrow = 5)

#positive impact on wins

grid.arrange(plt_TEAM_BATTING_H, plt_TEAM_BATTING_2B, plt_TEAM_BATTING_3B, plt_TEAM_BATTING_HR, plt_TEAM_BATTING_BB, plt_TEAM_BASERUN_SB,  plt_TEAM_BATTING_HBP,   plt_TEAM_PITCHING_SO,  plt_TEAM_FIELDING_DP,   ncol = 3, nrow = 5)

#negative impact on wins
grid.arrange(plt_TEAM_BATTING_SO,  plt_TEAM_BASERUN_CS,  plt_TEAM_PITCHING_H, plt_TEAM_PITCHING_HR,  plt_TEAM_FIELDING_E,  plt_TEAM_PITCHING_BB,    ncol = 3, nrow = 5)



```


## 3.Build  Models
To fit a multiple linear regression model with TARGET_WINS as the response variable all the other predictors as the explanatory variables except 'TEAM BASERUN CS' &  'TEAM BATTING HBP' as they have very low correlation with Wins:
We eliminate TEAM_BASERUN_CS

# 3.1 Manual elimination
```{r}
#Full Model

team.mod1 <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_BATTING_HBP +
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)
summary(team.mod1)

 #R-squared:  0.3192,  Adjusted R-squared:  0.3147
team.mod2 <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + 
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)

summary(team.mod2)
#R-squared:  0.3188,  Adjusted R-squared:  0.3149  
#Adjusted R-squared is higher than the original one.  


```

#3.2 Stepwise Regression
```{r}
library(MASS)
fit <- lm(formula = TARGET_WINS~TEAM_BATTING_H +  TEAM_BATTING_2B+  TEAM_BATTING_3B +
 TEAM_BATTING_HR + TEAM_BATTING_BB +  TEAM_BATTING_SO +  TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_BATTING_HBP + 
TEAM_PITCHING_H +  TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO +
TEAM_FIELDING_E + TEAM_FIELDING_DP,data =moneyballTraining)

# Stepwise Regression
step <- stepAIC(fit, direction="both")
step$anova # display results
summary(step)
#R-squared:  0.3186,  Adjusted R-squared:  0.3153 

par(mfrow=c(2, 2))
plot(step,, main="Stepwise Regression")
```
#Model with Stepwise Regression
TARGET_WINS = 23.67 + 0.048*TEAM_BATTING_H + -0.020*TEAM_BATTING_2B + 0.0625*TEAM_BATTING_3B + 
    0.0698*TEAM_BATTING_HR + 0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 0.029*TEAM_BASERUN_SB + 
    -0.0007*TEAM_PITCHING_H + 0.0029*TEAM_PITCHING_SO + -0.0206*TEAM_FIELDING_E + -0.121*TEAM_FIELDING_DP
    
#3.3 Backward elimination model   
```{r}
step.backward <- step(fit, direction="backward")
step$anova # display results
summary(step.backward)

#R-squared:  0.3186,  Adjusted R-squared:  0.3153
par(mfrow=c(2, 2))
plot(step.backward, main="Backward elimination")
```
#Model with Backward elimination
TARGET_WINS = 23.66 + 0.048*TEAM_BATTING_H + -0.020*TEAM_BATTING_2B + 
    0.062*TEAM_BATTING_3B + 0.0698*TEAM_BATTING_HR + 0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 
    0.029*TEAM_BASERUN_SB + -0.001*TEAM_PITCHING_H + 0.002*TEAM_PITCHING_SO + -0.021*TEAM_FIELDING_E + 
    -0.121*TEAM_FIELDING_DP

#3.4 forward Elimination
```{r}
forward.null <-lm(TARGET_WINS~1,data=moneyballTraining)
forward.full <-lm(TARGET_WINS~.,data=moneyballTraining)
step.forward<- step(forward.null, scope=list(lower=forward.null, upper=forward.full), direction="forward")

summary(step.forward)
# R-squared:  0.3188,  Adjusted R-squared:  0.3152
par(mfrow=c(2, 2))
plot(step.forward, main="Forward Elimination")
```
#Model with forward elimination
TARGET_WINS = 24.23 + 0.048*TEAM_BATTING_H + -0.021*TEAM_FIELDING_E + 
    0.029*TEAM_BASERUN_SB + -0.12*TEAM_FIELDING_DP + 0.015*TEAM_PITCHING_HR + 0.061*TEAM_BATTING_3B + 
    0.011*TEAM_BATTING_BB + -0.009*TEAM_BATTING_SO + 0.003*TEAM_PITCHING_SO + -0.0007*TEAM_PITCHING_H + 
    0.054*TEAM_BATTING_HR + -0.02*TEAM_BATTING_2B


##Backward elimination and Stepwise Regression Model has the best Adjusted R-squared value.
###So we would like select Backward elimination model  



##Adding the below simple regression analysis, we can remove if it does not make sense in final version. 
##-Suman

#3.5 Regression Analysis by removing high vif, and high p value predictors
```{r}
glimpse(moneyballTraining)

#Lets first break the training set into training and test sets.
set.seed(11)
samples <- sample(1:nrow(moneyballTraining), 0.75*nrow(moneyballTraining))
moneyballTraining <- moneyballTraining[samples,]
moneyballTest <- moneyballTraining[-samples,]
glimpse(moneyballTraining)
glimpse(moneyballTest)

#Let's consider ALL the variables ( except INDEX).
fit1 <- lm(TARGET_WINS ~ . -INDEX, data=moneyballTraining)
summary(fit1)

#Lets check for Multi-Collinearity - lets find vif value and drop those that has 
#got high vif (>5)
vifFit1 <- vif(fit1)
vifFit1

#sort by descending
sort(vifFit1, decreasing = T)

#These has got high vif value [ highly correlated results in multicolinearity ]
#TEAM_BATTING_HR TEAM_PITCHING_HR  TEAM_BATTING_BB TEAM_PITCHING_BB 
#36.901942        29.744427         6.815457         6.363007

fit2 <- lm(TARGET_WINS ~ .-INDEX -TEAM_BATTING_HR -TEAM_PITCHING_HR -TEAM_BATTING_BB -TEAM_PITCHING_BB, data=moneyballTraining)

#Lets now review the summary of this model, and look at the p-values now. Lets get rid of the
#variables with p-value > 0.05
summary(fit2)

# These are all got p value > 0.05, lets drop these in our next model
#TEAM_BATTING_3B   3.188e-02  1.879e-02   1.697  0.089971 
#TEAM_BATTING_SO   5.331e-05  2.521e-03   0.021  0.983129
#TEAM_BATTING_HBP  1.027e-01  8.655e-02   1.187  0.235449  
#TEAM_BATTING_2B  -1.584e-02  1.063e-02  -1.490  0.136465 

fit3 <- lm(TARGET_WINS ~ .-INDEX -TEAM_BATTING_HR -TEAM_PITCHING_HR -TEAM_BATTING_BB -TEAM_PITCHING_BB -TEAM_BATTING_3B, data=moneyballTraining)

summary(fit3)

#TEAM_BATTING_SO  -0.0016982  0.0023009  -0.738 0.4606 
#TEAM_BATTING_HBP  0.1004744  0.0865918   1.160 0.2461
#TEAM_PITCHING_H  -0.0004844  0.0004140  -1.170 0.2422
#TEAM_BATTING_2B  -0.0174378  0.0105980  -1.645 0.1001

#Lets remove TEAM_BATTING_SO, TEAM_BATTING_HBP,TEAM_PITCHING_H,TEAM_BATTING_2B
fit4 <- lm(TARGET_WINS ~ .-INDEX -TEAM_BATTING_HR -TEAM_PITCHING_HR -TEAM_BATTING_BB -TEAM_PITCHING_BB -TEAM_BATTING_3B -TEAM_BATTING_SO -TEAM_BATTING_HBP -TEAM_PITCHING_H -TEAM_BATTING_2B, data=moneyballTraining)

summary(fit4)

```

#3.6 RMSE - Root Mean Sqared Error (verification with test data)
```{r}
#Lets take our model, and apply it on the test dataset.
predicted.wins <- predict(fit4, newdata = moneyballTest)

#Lets calculate the RMSE
residuals <- moneyballTest$TARGET_WINS - predicted.wins
(rmse_test <- sqrt(mean(residuals^2)))
#high rmse [13.3875]

#lets put in ggplot
rmse.df <- data.frame(actual = moneyballTest$TARGET_WINS, predicted = predicted.wins)

dev.off()
ggplot(rmse.df, aes(x=actual, y = predicted)) + geom_point()

#hmmm. there appears to be few outliers
```

#3.7 Diagnostic plots, check for linearity, normality is justified for residuals ...
```{r}
#Check if linearity in residuals violated
plot(fit4, which=1)

#Observation: the red line is about flat, which indicates the linearity in residuals is good.


#Is residual variance is constant [ assumption of homo scedasticity is fine or not ]
plot(fit4, which=3)


#For normality of residuals
plot(fit4, which=2) #if most of the resids are on the straight line we are good to go.

#Lets check normality via ggplot for specific data element
# say, TEAM_BATTING_HR
d = data.frame(x=fit4$residuals, y = moneyballTraining$TEAM_BATTING_HR)
ggplot(d, aes(x,y)) + geom_point() + geom_smooth()

ggplot(d, aes(x)) + geom_histogram()

ggplot(d, aes(x)) + geom_density(color="red") + stat_function(fun=dnorm, args=list(mean=mean(d$x), sd=sd(d$x)), color="dark green") #residuals are normal.
```

#3.8 Evaluation
```{r}
moneyballEvaluation <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW1/moneyball-evaluation-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

#check how many na's for each column
apply(moneyballEvaluation, 2, function(x) sum(is.na(x)))

#Replacing Missing Values In dataset with column mean
for(i in 1:ncol(moneyballEvaluation)){
  moneyballEvaluation[is.na(moneyballEvaluation[,i]), i] <- mean(moneyballEvaluation[,i], na.rm = TRUE)
}

apply(moneyballEvaluation, 2, function(x) sum(is.na(x)))

eval.wins <- predict(fit4, newdata = moneyballEvaluation)

head(eval.wins)

#....what else ?

```


