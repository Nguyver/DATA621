---
title: "Critical Thinking Group 4 - HW3"
author: "Sreejaya, Suman, Vuthy"
date: "October 10, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

```{r, load libraries}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(car)
library(recommenderlab)
library(knitr)
library(Amelia)
library(PerformanceAnalytics)
library(robustbase)
library(BMA)
library(caret)
```

## Overview
The purpose of this project is to predict if a neighborhood will be at risk for high crime levels using binary logistic regression models. Below is a short description of the variables in the dataset.

- zn: proportion of residential land zoned for large lots (over 25000 square feet)
- indus: proportion of non-retail business acres per suburb
- chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0)
- nox: nitrogen oxides concentration (parts per 10 million)
- rm: average number of rooms per dwelling
- age: proportion of owner-occupied units built prior to 1940
- dis: weighted mean of distances to five Boston employment centers
- rad: index of accessibility to radial highways
- tax: full-value property-tax rate per $10,000
- ptratio: pupil-teacher ratio by town
- black: 1000 $(B_k - 0.63)^2$ where Bk is the proportion of blacks by town
- lstat: lower status of the population (percent)
- medv: median value of owner-occupied homes in $1000s
- target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

Dataset  
[Crime - Training data](https://github.com/Nguyver/DATA621-HW/blob/master/HW3/crime-training-data.csv)  
[Crime - Evaluation Data](https://github.com/Nguyver/DATA621-HW/blob/master/HW3/crime-evaluation-data.csv)

## Data Exploration

The dataset contains 466 observations and 14 variables. The response variable is the **target** variable. Below is a glimpse of the data. A quick look indicates that chas and target might be classification variables.
```{r read data}
crime.trn  <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW3/crime-training-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

crime.evl  <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW3/crime-evaluation-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

```{r}
glimpse(crime.trn)
```

Taking a closer look at the data with summary statistics, we can see that two values (chas, target) should be converted to factors.

```{r}
summary(crime.trn)
```

### Visually assessing missing values: 
The Amelia package has a plotting function missmap() that will plot the dataset and highlight missing values:
```{r}
missmap(crime.trn, main = "Missing values vs observed")
```
There are no missing values in the dataset. Lets plot the correlation between the variables.

```{r, warning=FALSE, message=FALSE}
cor.matrix <- cor(crime.trn[,1:ncol(crime.trn)])
chart.Correlation(cor.matrix, histogram=TRUE, pch=19)
```

From the above correlation matrix , the **target** variable seems to have correlation with 

  * zn    - proportion of residential land zoned for large lots 
  * indus - proportion of non-retail business acres per suburb 
  * nox   - nitrogen oxides concentration
  * age   - proportion of owner-occupied units built prior to 1940
  * dis   - weighted mean of distances to five Boston employment centers
  * rad   - index of accessibility to radial highways 
  * tax   - full-value property-tax rate per $10,000 
  * lstat - lower status of the population 


Lets look at a histogram for some of the predictor variable:

```{r}
g_zn <- ggplot(data = crime.trn) + geom_histogram(aes(x=zn), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_indus <- ggplot(data = crime.trn) + geom_histogram(aes(x=indus), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_nox <- ggplot(data = crime.trn) + geom_histogram(aes(x=nox), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_age <- ggplot(data = crime.trn) + geom_histogram(aes(x=age), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_dis <- ggplot(data = crime.trn) + geom_histogram(aes(x=dis), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_rad <- ggplot(data = crime.trn) + geom_histogram(aes(x=rad), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_tax <- ggplot(data = crime.trn) + geom_histogram(aes(x=tax), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))

g_lstat <- ggplot(data = crime.trn) + geom_histogram(aes(x=lstat), binwidth = 0.5) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
grid.arrange(g_zn, g_indus, g_nox, g_age,g_dis,g_rad,g_tax,g_lstat, ncol=2)
```  

Most of the neighborhoods have no residential land zoned for large lots.  

There is no specific categorizations noticed from the above predictor variables.  

## Data Preparation

Convert the *chas* and *target* variables into factors:

```{r echo=TRUE}
crime.trn$chas <- as.factor(crime.trn$chas)
crime.trn$target <- as.factor(crime.trn$target)
```

For ZN variable, 339/466 are zeros. We are going to create a new variable **zn_ind** as indicator for residential zones containing large lots (land size over 25,000 sq.ft as 1)

```{r}
#crime.trn$zn_ind <- ifelse(crime.trn$zn > 0, 1, 0)
crime.trn$zn <- ifelse(crime.trn$zn > 0, 1, 0)
table(crime.trn$zn)
```

Check for Multicolinearity among the predictor variables and remove those with excessive correlation among the explanatory variables. 

The below is the VIF values, lets get rid of those that has got VIF > 10.
 
```{r}
fit <- glm(target ~ . , data=crime.trn, family = binomial)

#Lets check for Multi-Collinearity - lets find vif value and drop those that has 
vifFit1 <- vif(fit)

#sort by descending
vif.df <- as.data.frame(sort(vifFit1, decreasing = T))
names(vif.df) <- c('VIF')
kable(vif.df)
```

From the above table, we do not see multi-colinearity (with VIF > 10) among the predictors.  

We will randomly split our dataset into training (80%) and test (20%).

```{r echo=TRUE}
set.seed(999) 
s=sample(1:nrow(crime.trn),0.80*nrow(crime.trn)) 
crime.train=crime.trn[s,] 
crime.test=crime.trn[-s,]
```


## Build Models

The below are the few different approaches we will try to build the models:

  1. Stepwise Backward
  2. Stepwise Forward
  3. Manual
  4. Bayesian

### 1. Backward elimination method 

With backwards elimination, we start with full set of parameters and iteratively reduce the numbers of parameters using AIC.

```{r}
 fullmodel = stats::glm(target ~ ., family=binomial(), data =crime.train)
 summary(fullmodel)
```

```{r}
backwards.model = step(fullmodel, trace = FALSE) #Backwards selection is the default
backwards.formula <- formula(backwards.model)
backwards.formula
summary(backwards.model) 

par(mfrow=c(2, 2))
graphics::plot(backwards.model)
```

### 2. Forward elimination method

With forward elimination, we start with an empty candidate set of parameters and iteratively add variables using AIC.

```{r}
nothing <- glm(target ~ 1,family=binomial,data =crime.train, trace = FALSE)
summary(nothing)
```

```{r}
forwards.model = step(nothing,
scope=list(lower=formula(nothing),upper=formula(fullmodel)), direction="forward", trace = FALSE)
forwards.formula <- formula(forwards.model)
forwards.formula
summary(forwards.model)

par(mfrow=c(2, 2))
graphics::plot(backwards.model)
```
From the above two models we can see that zn,& age are not statistically significant.
As for the statistically significant variables, rad & nox have  a strong positive association of crime rate while tax has a negative coefficient, suggests as all other variables being equal as tax increases crime rate decreases.


Both Forward and backward elimination models came up with the same model. We next remove variables of low significance. We will remove Zn and age from the above models.

```{r}
manual.model <- glm(target ~ nox + rad + tax + dis,family=binomial(link='logit'),data=crime.train, trace = FALSE)
summary(manual.model)
```

We will remove distance from the above model since the p value is not significant. Now the new model:
```{r}
manual.final <- glm(target ~ nox + rad + tax ,family=binomial(link='logit'),data=crime.train, trace = FALSE)
summary(manual.final)
```
A unit increase in index of accessibility to radial highways increses the log odds by 0.56. Also unit increase in nitrogen oxides concentration increases the log odds by 33.03, while increase in tax rate reduces the log odds by 0.008.

```{r}
par(mfrow=c(2, 2))
graphics::plot(manual.final)
```

In the residuals Vs Fitted graph, the red line is not flat, which indicates the linearity in residuals is not true. In the scale-location graph as well, the red line is not flat, which indicates that residual variance is not constant [homo scadasticity assumption]. The Normal Q-Q graph indicates that the most of the residuals are on the straight line.However, the Residual Vs Leverage plot has the redline not alligned with gray dotted line, this indicates that the assumption of standardized residuals centered around zero is NOT true here.

### 3.Bayesian Approach
```{r}
output <- bic.glm(target ~., data = crime.train, glm.family = "binomial")
summary(output)

# Posterior probability of each of 11  models (rest very small by
# comparison, so are omitted, change value of OR to see them)
 output$postprob
 output$label

# For each of 8 variables, probability they should be in the model
 output$names
 output$probne0
 
 imageplot.bma(output)
 output$postmean
```
From the above resuls it is clear nitrogen oxides concentration(nox), accessibility to radial highways(rad) and property-tax rate(tax) are the 3 variables -probability they should be in the model
The model is 
target ~ nox+rad+tax


## Select Models

### 1.  anova() function on the model to analyze the table of deviance
```{r}
anova(manual.final, test="Chisq")
```
The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better.
Nitrogen oxides concentration is the least deviation, so this variable can be dropped from the model.

### 2. Specificity and Sensitivity

```{r}
fitted.results <- predict(manual.final,newdata=subset(crime.test,select=c(4,8,9)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
# table(fitted.results)
# table(crime.test$target)

crime.test$predicted <- fitted.results
crime.test$predicted <- factor(crime.test$predicted)

#length(fitted.results)

#summary(crime.test)
#nrow(crime.test)
```

```{r}
# glimpse(crime.test)
# round(sensitivity(crime.test$predicted, 
#             crime.test$target, 
#             positive="1"), 4)
# 
# round(specificity(crime.test$predicted, 
#                    crime.test$target, 
#                    negative="0"),4)

cnfMtx <- confusionMatrix(crime.test$predicted, crime.test$target, positive = "1")

cnfMtx[3]
cnfMtx[4]
```



### 3. AUC 

```{r}

```


## Predictions

```{r predictions}
crime.evl$chas <- as.factor(crime.evl$chas)

crime.prd <- predict(manual.final,newdata=subset(crime.evl,select=c(4,8,9)),type='response')
crime.prd <- ifelse(crime.prd > 0.5,1,0)

crime.evl$predicted <- crime.prd
crime.evl$predicted <- factor(crime.evl$predicted)

kable(crime.evl)
```

## Appendix