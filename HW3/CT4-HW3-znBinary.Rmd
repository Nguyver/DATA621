---
title: "Critical Thinking Group 4 - HW3"
author: "Sreejaya, Suman, Vuthy"
date: "October 10, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

```{r, load libraries}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(e1071)
library(car)
library(recommenderlab)
library(knitr)
library(Amelia)
library(PerformanceAnalytics)
library(robustbase)
library(BMA)
library(caret)
library(pROC)
```

## Overview
The purpose of this project is to predict if a neighborhood will be at risk for high crime levels using binary logistic regression models. Below is a short description of the variables in the dataset.

- zn: proportion of residential land zoned for large lots (over 25000 square feet)
- indus: proportion of non-retail business acres per suburb
- chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0)
- nox: nitrogen oxides concentration (parts per 10 million)
- rm: average number of rooms per dwelling
- age: proportion of owner-occupied units built prior to 1940
- dis: weighted mean of distances to five Boston employment centers
- rad: index of accessibility to radial highways
- tax: full-value property-tax rate per $10,000
- ptratio: pupil-teacher ratio by town
- black: 1000 $(B_k - 0.63)^2$ where Bk is the proportion of blacks by town
- lstat: lower status of the population (percent)
- medv: median value of owner-occupied homes in $1000s
- target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

Dataset  
[Crime - Training data](https://github.com/Nguyver/DATA621-HW/blob/master/HW3/crime-training-data.csv)  
[Crime - Evaluation Data](https://github.com/Nguyver/DATA621-HW/blob/master/HW3/crime-evaluation-data.csv)

## Data Exploration

The dataset contains 466 observations and 14 variables. The response variable is the **target** variable. Below is a glimpse of the data. A quick look indicates that chas and target might be classification variables.
```{r read data}
crime.trn  <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW3/crime-training-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)

crime.evl  <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW3/crime-evaluation-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

```{r}
glimpse(crime.trn)
```

Taking a closer look at the data with summary statistics, we can see that two values (chas, target) should be converted to factors.

```{r}
summary(crime.trn)
```

### Visually assessing missing values: 
The Amelia package has a plotting function missmap() that will plot the dataset and highlight missing values:
```{r}
missmap(crime.trn, main = "Missing values vs observed")
```
There are no missing values in the dataset. Lets plot the correlation between the variables.

```{r, warning=FALSE, message=FALSE}
cor.matrix <- cor(crime.trn[,1:ncol(crime.trn)])
chart.Correlation(cor.matrix, histogram=TRUE, pch=19)
```

From the above correlation matrix , the **target** variable seems to have correlation with 

  * zn    - proportion of residential land zoned for large lots 
  * indus - proportion of non-retail business acres per suburb 
  * nox   - nitrogen oxides concentration
  * age   - proportion of owner-occupied units built prior to 1940
  * dis   - weighted mean of distances to five Boston employment centers
  * rad   - index of accessibility to radial highways 
  * tax   - full-value property-tax rate per $10,000 
  * lstat - lower status of the population 


Lets look at each of the predictor variable's data:


```{r}
g_zn <- ggplot(data = crime.trn) + geom_histogram(aes(x=log(zn))) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```

```{r}
g_indus <- ggplot(data = crime.trn) + geom_histogram(aes(x=indus)) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```


```{r}
g_nox <- ggplot(data = crime.trn) + geom_histogram(aes(x=nox)) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```


```{r}
g_age <- ggplot(data = crime.trn) + geom_histogram(aes(x=age)) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```


```{r}
g_dis <- ggplot(data = crime.trn) + geom_histogram(aes(x=dis)) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```

```{r}
g_rad <- ggplot(data = crime.trn) + geom_histogram(aes(x=rad)) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```

```{r}
g_tax <- ggplot(data = crime.trn) + geom_histogram(aes(x=log(tax))) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
```


```{r}
g_lstat <- ggplot(data = crime.trn) + geom_histogram(aes(x=lstat)) +  theme(axis.text=element_text(size=8), axis.title=element_text(size=8))
grid.arrange(g_zn, g_indus, g_nox, g_age,g_dis,g_rad,g_tax,g_lstat, ncol=2)
```  


From the above, it appears like majority of the neighborhoods have no residential land zoned for large lots. And the buildings with age > 100 are mentioned as 100 in the above. We could not derive a specific categorization in the other predictors.

## Data Preparation

### Factorize Variables:

Convert the *chas* and *target* variables into factors:

```{r echo=TRUE}
crime.trn$chas <- as.factor(crime.trn$chas)
crime.trn$target <- as.factor(crime.trn$target)
```

For ZN variable, 339/466 are zeros. We are going to create a new variable **zn_ind** as indicator for residential zones containing large lots (land size over 25,000 sq.ft as 1)

```{r}
crime.trn$zn_ind <- ifelse(crime.trn$zn > 0, 1, 0)
#crime.trn$zn <- ifelse(crime.trn$zn > 0, 1, 0)
table(crime.trn$zn_ind)
```

### Check for Multicolinearity in the predictors:

Check for Multicolinearity among the predictor variables and remove those with excessive correlation among the explanatory variables. 

The below is the VIF values, lets get rid of those that has got VIF > 10.
 
```{r}
fit <- glm(target ~ . , data=crime.trn, family = binomial)

#Lets check for Multi-Collinearity - lets find vif value and drop those that has 
vifFit1 <- vif(fit)

#sort by descending
vif.df <- as.data.frame(sort(vifFit1, decreasing = T))
names(vif.df) <- c('VIF')
kable(vif.df)
```

From the above table, we do not see multi-colinearity (with VIF > 10) among the predictors.  
### Split the dataset into training and test:

We will randomly split our dataset into training (80%) and test (20%).

```{r echo=TRUE}
set.seed(999) 
s=sample(1:nrow(crime.trn),0.80*nrow(crime.trn)) 
crime.train=crime.trn[s,] 
crime.test=crime.trn[-s,]
```

Number of observations in *training* dataset is `r nrow(crime.train)`
Number of observations in *test* dataset is `r nrow(crime.test)`

## Build Models

The below are the few different approaches we will try to build the models:

  1. Stepwise Backward
  2. Stepwise Forward
  3. Manual
  4. Bayesian

### 1. Backward elimination method 

With backwards elimination, we start with full set of parameters and iteratively reduce the numbers of parameters using AIC.

```{r}
 fullmodel = stats::glm(target ~ ., family=binomial(), data =crime.train)
 summary(fullmodel)
```

```{r}
backwards.model = step(fullmodel, trace = FALSE) #Backwards selection is the default
backwards.formula <- formula(backwards.model)
backwards.formula
summary(backwards.model) 

par(mfrow=c(2, 2))
graphics::plot(backwards.model)
```

### 2. Forward elimination method

With forward elimination, we start with an empty candidate set of parameters and iteratively add variables using AIC.

```{r}
nothing <- glm(target ~ 1,family=binomial,data =crime.train, trace = FALSE)
summary(nothing)
```

```{r}
forwards.model = step(nothing,
scope=list(lower=formula(nothing),upper=formula(fullmodel)), direction="forward", trace = FALSE)
forwards.formula <- formula(forwards.model)
forwards.formula
summary(forwards.model)

par(mfrow=c(2, 2))
graphics::plot(backwards.model)
```
From the above two models we can see that zn & age are not statistically significant.
As for the statistically significant variables, rad & nox have  a strong positive association of crime rate while tax has a negative coefficient, suggests as all other variables being equal as tax increases crime rate decreases.

### 3. Manual
Both Forward and backward elimination models produced the same model. Using the model obtained from backwards/forwards elimination, we next remove variables of low significance. We will remove Zn and age from the above models.

```{r}
manual.model <- glm(target ~ nox + rad + tax + dis,family=binomial(link='logit'),data=crime.train, trace = FALSE)
summary(manual.model)
```

We will remove distance from the above model since the p value is not significant. Now the new model:
```{r}
manual.final <- glm(target ~ nox + rad + tax ,family=binomial(link='logit'),data=crime.train, trace = FALSE)
summary(manual.final)
```
A unit increase in index of accessibility to radial highways increses the log odds by 0.56. Also unit increase in nitrogen oxides concentration increases the log odds by 33.03, while increase in tax rate reduces the log odds by 0.008.

```{r}
par(mfrow=c(2, 2))
graphics::plot(manual.final)
```

In the residuals Vs Fitted graph, the red line is not flat, which indicates the linearity in residuals is not true. In the scale-location graph as well, the red line is not flat, which indicates that residual variance is not constant [homo scadasticity assumption]. The Normal Q-Q graph indicates that the most of the residuals are on the straight line.However, the Residual Vs Leverage plot has the redline not alligned with gray dotted line, this indicates that the assumption of standardized residuals centered around zero is NOT true here.

### 4.Bayesian Approach
```{r}
output <- bic.glm(target ~., data = crime.train, glm.family = "binomial")
summary(output)

# Posterior probability of each of 11  models (rest very small by
# comparison, so are omitted, change value of OR to see them)
 output$postprob
 output$label

# For each of 8 variables, probability they should be in the model
 output$names
 output$probne0
 
 imageplot.bma(output)
 output$postmean
```
From the above resuls it is clear nitrogen oxides concentration(nox), accessibility to radial highways(rad) and property-tax rate(tax) are the 3 variables -probability they should be in the model
The model is 
target ~ nox+rad+tax


## Select Models

### 1.  anova() function on the model to analyze the table of deviance
```{r}
anova(manual.final, test="Chisq")
```
The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better.
Nitrogen oxides concentration is the least deviation, so this variable can be dropped from the model.

### 2. Performance measures:

Sensitivity is basically the ability of the model to capture all positives.  And Specificity is the ability of the model to capture all negatives.

Sensitivity = $$\frac{TP}{TP+FN}$$
Specificity = $$\frac{TN}{TN+FP}$$

For an ideal model, the predictions will be perfect - meaning the 'accuracy, sensitivity and specificity' will all be 1 where as the mis-classification error will be zero. In practical scenarios we would like to have the sensitivity and spcificity as close to 1 as possible. Lets see which model provides us with the better measures:

*Manual Model from Forward & Backward Steps* 

Score should be high when outcome is 1 and low when outcome is 0. Lets visualize how our binary response is behaving with respect to the score that we obtained.

```{r}
#formula
formula(manual.final)
summary(manual.final)$coefficients[,4] 
#fitted.results <- predict(manual.final,newdata=subset(crime.test,select=c(4,8,9)),type='response')
#fitted.results <- ifelse(fitted.results > 0.5,1,0)

crime.test$score <- predict(manual.final,newdata=subset(crime.test,select=c(4,8,9)),type='response')

ggplot(crime.test, aes(y=target, x=score, color=factor(target))) + geom_point() + geom_jitter()

```

We can see that the response 0 is bunched around low scores and response 1 is bunched around high scores. However, there is also overlap as well across some scores. We need to find a cutoff in the score so as to reach our target here.

Based on our previous homework, these are some properties of the cutoff/threshold:

All the predicted values above cutoff will be 1 
All the predicted values below cutoff will be 0  
Response values above cutoff(predicted 1) which are 1 in reality will be noted as TP 
Response values above cutoff(predicted 1) which are 0 in reality will be noted as FP 
Response values below cutoff(predicted 0) which are 1 in reality will be noted as FN 
Response values below cutoff(predicted 0) which are 0 in reality will be noted as TN

Based on our visualization, it appears like 0.50 could be a better cutoff.
```{r}
cutoff=0.5
crime.test$predicted=as.numeric(crime.test$score>cutoff) 
TP=sum(crime.test$predicted==1 & crime.test$target==1) 
FP=sum(crime.test$predicted==1 & crime.test$target==0) 
FN=sum(crime.test$predicted==0 & crime.test$target==1) 
TN=sum(crime.test$predicted==0 & crime.test$target==0)

# lets also calculate total number of real positives and negatives in the data 
P=TP+FN 
N=TN+FP
total = P + N
```


```{r}
#glimpse(crime.test)
confusionMatrix(factor(crime.test$predicted), factor(crime.test$target), positive = "1")

sensitivity <- round(sensitivity(factor(crime.test$predicted),crime.test$target, positive="1"), 4)

specificity <- round(specificity(factor(crime.test$predicted),crime.test$target, negative="0"),4)

#accuracy = (TP+TN)/(P+N)
accuracy <- (TP + TN) / (P + N)

cnfMtx <- confusionMatrix(crime.test$predicted, crime.test$target, positive = "1")

```



### 3. AUC 

```{r, eval=TRUE, echo=TRUE}
(roc <- roc(factor(predicted)~as.numeric(target),data=crime.test, plot=FALSE, ci=TRUE))
graphics::plot(roc, legacy.axes = TRUE, col="blue", lwd=3)
auc(factor(predicted)~as.numeric(target),crime.test)
```


## Predictions

```{r predictions}
crime.evl$chas <- as.factor(crime.evl$chas)

crime.prd <- predict(manual.final,newdata=subset(crime.evl,select=c(4,8,9)),type='response')
crime.prd <- ifelse(crime.prd > 0.5,1,0)

crime.evl$predicted <- crime.prd
crime.evl$predicted <- factor(crime.evl$predicted)

kable(crime.evl)
```

## Appendix

```{r eval=FALSE, echo=TRUE}

```