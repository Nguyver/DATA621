---
title: "Critical Thinking Group 4 - HW2"
author: "Sreejaya, Suman, Vuthy"
date: "September 27, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```


```{r}
library(dplyr)
library(ggplot2)
library(knitr)
library(e1071)
library(caret)
library(pROC)
```

## Overview
In this assignment, we will work through various classification metrics. We will implement the various functions to calculate the metrics as well as explore functions from various packages that perform equivalent calculations. We will also generate graphical outputs used to evaluate the classification models.  

**Dataset**  
[Classification Output](https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW2/classification-output-data.csv)  

## Data Exploration
The *classification output* dataset contains 181 observations and 11 variables. All 11 variables are numeric including the 3 variables we are interested in:

- **class**: the actual class for the observations
- **score.class**: the predicted class for the observations (based on a threshold of 0.5)
- **scored.probability**: the predicted probability of success for the observation
  
<<<<<<< HEAD
=======
```{r}
classification.orig  <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW2/classification-output-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

```{r}
glimpse(classification.orig)
```
## Data Preparation

A confusion matrix is typically used to describe the performance of a classification model. The matrix contains 2 rows and 2 columns that reports the number of true negative, false negative, false positive and true positive. The values in the dataset needs to be factors in order to be classified using the table() function.  

Since all the variables in the dataset were numeric, *class* and *score.class* variables were converted to factors.

```{r}
class.df <- classification.orig

#Convert numbers to factors
class.df$class <- factor(class.df$class)
class.df$scored.class <- factor(class.df$scored.class)
```

```{r}
glimpse(class.df)
summary(class.df[,9:11])
```

The resulting confusion matrix where rows represents the Actual class and columns represent the predicted class.
```{r echo=FALSE}
cnf.mtx <- table(class.df$class, class.df$scored.class)
knitr::kable(cnf.mtx, caption="Confusion Matrix")
```

## Classification Metrics
Please see Appendix (Appendix section) for custom implementation details.

  - Accuracy = How often is the classifier correct? $$\frac{TP+TN}{TP+FP+TN+FN}$$
  - Classification Error Rate = How often is the classifier incorrect? $$\frac{FP+FN}{TP+FP+TN+FN}$$
  - Precision = When the classifier predicts yes, how often is it correct? $$\frac{TP}{TP+FP}$$
  - Sensitivity = When the actual value is yes, how often does it predict yes?$$\frac{TP}{TP+FN}$$
  - Specificity = When the actual value is no, how often does it predict no? $$\frac{TN}{TN+FP}$$
  - F1 Score = Weighted average of the sensitivity and precision $$\frac{2 X Precision X Sensitivity}{Precision + Sensitivity}$$
  
Metrics definition sourced from **[Data School](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)**

```{r}
accuracy <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4; 
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- (confusion[TP]+confusion[TN])/(confusion[TP]+confusion[FP]+confusion[TN]+confusion[FN])
  
  return(value)
}

classErrorRate <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- (confusion[FP]+confusion[FN])/(confusion[TP]+confusion[FP]+confusion[TN]+confusion[FN])
  
  return(value)
}

precision <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TP]/(confusion[TP]+confusion[FP])
  
  return(value)
}

sensitivity <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TP]/(confusion[TP]+confusion[FN])
  
  return(value)
}

specificity <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TN]/(confusion[TN]+confusion[FP])
  
  return(value)
}

f1Score <- function(data, actual.idx=9, predict.idx=10){
  confusion <- table(data[,actual.idx], data[,predict.idx])
  
  value <- (2*precision(data)*sensitivity(data))/(precision(data)+sensitivity(data))
  
  return(value)
}
```

```{r}
accuracy <- accuracy(class.df)
Sn <- sensitivity(class.df)
Sp <- specificity(class.df)
ClsErrRate <- classErrorRate(class.df)
f1 <- f1Score(class.df)
precsn <- precision(class.df)

metrics <- data.frame(Metric=c(), Value=c())
metrics <- rbind(metrics, data.frame(Metric = "Accuracy", Value = round(accuracy,4)))
metrics <- rbind(metrics, data.frame(Metric = "Sn(True Positive Rate)", Value = round(Sn,4)))
metrics <- rbind(metrics, data.frame(Metric = "Sp(True Negative Rate)", Value = round(Sp,4)))
metrics <- rbind(metrics, data.frame(Metric = "ClassErrorRate", Value = round(ClsErrRate,4)))
metrics <- rbind(metrics, data.frame(Metric = "F1Score", Value = round(f1,4)))
metrics <- rbind(metrics, data.frame(Metric = "Precision", Value = round(precsn,4)))

kable(metrics, caption="Confusion Matrix Metrics using custom functions")
```


  Before we move on, let's consider a question that was asked: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1.

 F1 score reaches its best value at 1 and worst at 0.since both precision and sensitivity are in between 0 and 1, we know $$0 < p*s <=1$$ and $$ 2*p*s <= p + s$$ (maximum value p and s can have is 1 ), so $$ 2*p*s / p + s$$ can have values in between 0 and 1.
 
## Graphical Evaluation

**ROC Curve**

 We will use the *ROC Curve* to characterize the sensitivity/specificity tradeoffs for our binary classifier (class in our example) . We will have a series of threshold levels (a.k.a Cut-Offs)  between zero and one in inrements of 0.01, and then with the help of the probability column (scored.probability) we will find out the sensitivity and specificity at each of the threshold levels. Basically, the cases with 'scored.probability' equal to or above the 'threshold' are classified as positive,and the ones below the threshold are classified as negative.So, different thresholds give different levels of sensitivity and specificity. 
 
A high threshold is generally results in labelling more negative cases and low threshold produces more postive labels. 
 
And the *ROC Curve* plots the **true positive rate against the false positive rate**.

Here is the RoC Curve along with few threshold rows from our custom R function **getRoCData**:


>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
```{r}
getRoCData <- function(data)
{
  rocResults <- data.frame(Threshold=c(), Sn=c(), Sp=c())
  
  thresholds <- seq(0, 1, by=0.01)
  for(threshold in thresholds)
  {
    data <- data %>% mutate(predicted = as.numeric(scored.probability >= threshold))
    data$predicted  <- factor( data$predicted , levels=c(0,1) )
    
    Sn <- sensitivity(data, actual.idx =  9, predict.idx =  12)
    Sp <- specificity(data, actual.idx =  9, predict.idx =  12)
    
    rocResults <- rbind(rocResults, data.frame(Threshold=c(threshold), Sn=c(Sn), Sp=c(Sp)))
  }
  
  auc <- cumsum(rocResults$Sn - (1-rocResults$Sp))*0.01
 
  rocPlot <- ggplot(rocResults, aes(x=1-Sp, y=Sn) ) + geom_line(size = 1, alpha = 0.5) +
    geom_point(data=rocResults[rocResults$Threshold == 0.5,], 
               aes(x=1-Sp, y=Sn), colour="orange") +
    geom_abline(slope=1, intercept=0, colour="blue", size=0.50) +
          xlim(0, 1) + ylim(0, 1) +
    labs(title="ROC Curve",
         x = "False Positive Rate ( 1 - Sp) ",
         y = "True Positive Rate (Sn)")

  results <- list(rocResults = rocResults, plot = rocPlot, auc = auc)

  return(results)
}

result <- getRoCData(class.df)
knitr::kable(head(result$rocResults,20))
result$plot
result$auc
```


Our RoC curve follows the left-hand border and then the top border of the ROC space, so it indicates a good test.


## Appendix 


###1. Download the classification output data set
```{r, eval=FALSE, echo=TRUE}
classification.orig  <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW2/classification-output-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

```{r}
glimpse(classification.orig)
```
## Data Preparation

A confusion matrix is typically used to describe the performance of a classification model. The matrix contains 2 rows and 2 columns that reports the number of true negative, false negative, false positive and true positive. The values in the dataset needs to be factors in order to be classified using the table() function.  

Since all the variables in the dataset were numeric, *class* and *score.class* variables were converted to factors.

```{r}
class.df <- classification.orig

#Convert numbers to factors
class.df$class <- factor(class.df$class)
class.df$scored.class <- factor(class.df$scored.class)
```

```{r}
glimpse(class.df[,9:11])
summary(class.df[,9:11])
```

The resulting confusion matrix where rows represents the Actual class and columns represent the predicted class.
```{r echo=FALSE}
cnf.mtx <- table(class.df$class, class.df$scored.class)
knitr::kable(cnf.mtx, caption="Confusion Matrix")
```

## Classification Metrics
Please see Appendix (Appendix section) for custom implementation details.

  - Accuracy = How often is the classifier correct? $$\frac{TP+TN}{TP+FP+TN+FN}$$
  - Classification Error Rate = How often is the classifier incorrect? $$\frac{FP+FN}{TP+FP+TN+FN}$$
  - Precision = When the classifier predicts yes, how often is it correct? $$\frac{TP}{TP+FP}$$
  - Sensitivity = When the actual value is yes, how often does it predict yes?$$\frac{TP}{TP+FN}$$
  - Specificity = When the actual value is no, how often does it predict no? $$\frac{TN}{TN+FP}$$
  - F1 Score = Weighted average of the sensitivity and precision $$\frac{2 X Precision X Sensitivity}{Precision + Sensitivity}$$
  
Metrics definition sourced from **[Data School](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)**

```{r}
accuracy <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4; 
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- (confusion[TP]+confusion[TN])/(confusion[TP]+confusion[FP]+confusion[TN]+confusion[FN])
  
  return(value)
}

classErrorRate <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- (confusion[FP]+confusion[FN])/(confusion[TP]+confusion[FP]+confusion[TN]+confusion[FN])
  
  return(value)
}

precision <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TP]/(confusion[TP]+confusion[FP])
  
  return(value)
}

sensitivity <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TP]/(confusion[TP]+confusion[FN])
  
  return(value)
}

specificity <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TN]/(confusion[TN]+confusion[FP])
  
  return(value)
}

f1Score <- function(data, actual.idx=9, predict.idx=10){
  confusion <- table(data[,actual.idx], data[,predict.idx])
  
  value <- (2*precision(data)*sensitivity(data))/(precision(data)+sensitivity(data))
  
  return(value)
}
```

```{r}
accuracy <- accuracy(class.df)
Sn <- sensitivity(class.df)
Sp <- specificity(class.df)
ClsErrRate <- classErrorRate(class.df)
f1 <- f1Score(class.df)
precsn <- precision(class.df)

metrics <- data.frame(Metric=c(), Value=c())
metrics <- rbind(metrics, data.frame(Metric = "Accuracy", Value = round(accuracy,4)))
metrics <- rbind(metrics, data.frame(Metric = "Sn(True Positive Rate)", Value = round(Sn,4)))
metrics <- rbind(metrics, data.frame(Metric = "Sp(True Negative Rate)", Value = round(Sp,4)))
metrics <- rbind(metrics, data.frame(Metric = "ClassErrorRate", Value = round(ClsErrRate,4)))
metrics <- rbind(metrics, data.frame(Metric = "F1Score", Value = round(f1,4)))
metrics <- rbind(metrics, data.frame(Metric = "Precision", Value = round(precsn,4)))

kable(metrics, caption="Confusion Matrix Metrics using custom functions")
```

Please see the confusion matrix from caret package.
We compared the caret confusionmatrix with the one we created; all the measures are matched with the custom functions. Specificity and sensitivity are also same as we calculated.

```{r}
confusionMatrix(class.df$scored.class, class.df$class, positive = "1")
```



  Before we move on, let's consider a question that was asked: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1.

 F1 score reaches its best value at 1 and worst at 0.since both precision and sensitivity are in between 0 and 1, we know $$0 < p*s <=1$$ and $$ 2*p*s <= p + s$$ (maximum value p and s can have is 1 ), so $$ 2*p*s / p + s$$ can have values in between 0 and 1.
 
## Graphical Evaluation

**ROC Curve**

 We will use the *ROC Curve* to characterize the sensitivity/specificity tradeoffs for our binary classifier (class in our example) . We will have a series of threshold levels (a.k.a Cut-Offs)  between zero and one in inrements of 0.01, and then with the help of the probability column (scored.probability) we will find out the sensitivity and specificity at each of the threshold levels. Basically, the cases with 'scored.probability' equal to or above the 'threshold' are classified as positive,and the ones below the threshold are classified as negative.So, different thresholds give different levels of sensitivity and specificity. 
 
A high threshold is generally results in labelling more negative cases and low threshold produces more postive labels. 
 
And the *ROC Curve* plots the **true positive rate against the false positive rate**.

Here is the RoC Curve along with few threshold rows from our custom R function **getRoCData**:


```{r}
getRoCData <- function(data)
{
  rocResults <- data.frame(Threshold=c(), Sn=c(), Sp=c())
  
  thresholds <- seq(0, 1, by=0.01)
  for(threshold in thresholds)
  {
    data <- data %>% mutate(predicted = as.numeric(scored.probability >= threshold))
    data$predicted  <- factor( data$predicted , levels=c(0,1) )
    
    Sn <- sensitivity(data, actual.idx =  9, predict.idx =  12)
    Sp <- specificity(data, actual.idx =  9, predict.idx =  12)
    
    rocResults <- rbind(rocResults, data.frame(Threshold=c(threshold), Sn=c(Sn), Sp=c(Sp)))
  }
  
  auc <- cumsum(rocResults$Sn - (1-rocResults$Sp))*0.01
 
  rocPlot <- ggplot(rocResults, aes(x=1-Sp, y=Sn) ) + geom_line(size = 1, alpha = 0.5) +
    geom_point(data=rocResults[rocResults$Threshold == 0.5,], 
               aes(x=1-Sp, y=Sn), colour="orange") +
    geom_abline(slope=1, intercept=0, colour="blue", size=0.50) +
          xlim(0, 1) + ylim(0, 1) +
    labs(title="ROC Curve",
         x = "False Positive Rate ( 1 - Sp) ",
         y = "True Positive Rate (Sn)")

  results <- list(rocResults = rocResults, plot = rocPlot, auc = auc)

  return(results)
}

result <- getRoCData(class.df)
knitr::kable(head(result$rocResults,20))
result$plot
result$auc
```


Our RoC curve follows the left-hand border and then the top border of the ROC space, so it indicates a good test.


## Appendix 


###1. Download the classification output data set
```{r, eval=FALSE, echo=TRUE}
classification.orig  <- read.csv("https://raw.githubusercontent.com/Nguyver/DATA621-HW/master/HW2/classification-output-data.csv", header=TRUE, sep=",", stringsAsFactors = FALSE)
```

###2. The data set has three key columns we will use:

- **class**: the actual class for the observations
- **score.class**: the predicted class for the observations (based on a threshold of 0.5)
- **scored.probability**: the predicted probability of success for the observation

Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you understand the output. In particular, do the rows represent the actual or predicted class? The Columns?  

In our case, the columns are the predictions and the rows are the actuals.  
```{r, eval=FALSE, echo=TRUE}
class.df <- classification.orig

#Convert numbers to factors
class.df$class <- factor(class.df$class)
class.df$scored.class <- factor(class.df$scored.class)

glimpse(class.df)
summary(class.df)

cnf.mtx <- table(class.df$class, class.df$scored.class)
knitr::kable(cnf.mtx)

# Rows represent Actual class and the columns represent Predicted class.

# Verifying Table results  
#               = Predict-Actual  
#True Positive  = 1-1  
#False Positive = 1-0  
#True Negative  = 0-0  
#False Negative = 0-1  
#sum(class.df$class == class.df$scored.class & class.df$class ==1) #TP (2,2)
#sum(class.df$class != class.df$scored.class & class.df$class ==0) #FN (1,2)

#sum(class.df$class == class.df$scored.class & class.df$class ==0) #TN (1,1)
#sum(class.df$class != class.df$scored.class & class.df$class ==1) #FP (2,1)
```

###3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.
$$Accuracy = \frac{TP+TN}{TP+FP+TN+FN}$$
 Accuracy: Overall, how often is the classifier correct?
<<<<<<< HEAD
```{r, eval=FALSE,echo=TRUE}
=======
```{r, eval=FALSE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
accuracy <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- (confusion[TP]+confusion[TN])/(confusion[TP]+confusion[FP]+confusion[TN]+confusion[FN])
  
  return(value)
}

#Example
accuracy(class.df,9,10)
```

###4. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.
$$Classification Error Rate = \frac{FP+FN}{TP+FP+TN+FN}$$
Verify that you get an accuracy and error rate that sums to one.

Error Rate: Overall, how often is it wrong?
<<<<<<< HEAD
```{r, eval=FALSE,echo=TRUE}
=======
```{r, eval=FALSE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
classErrorRate <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- (confusion[FP]+confusion[FN])/(confusion[TP]+confusion[FP]+confusion[TN]+confusion[FN])
  
  return(value)
}
classErrorRate(class.df)
classErrorRate(class.df)+accuracy(class.df)
```

###5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.
$$Precision = \frac{TP}{TP+FP}$$

```{r, eval=FALSE, echo=TRUE}
precision <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TP]/(confusion[TP]+confusion[FP])
  
  return(value)
}

precision(class.df)
```

###6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.
$$Sensitivity = \frac{TP}{TP+FN}$$
```{r, eval=FALSE, echo=TRUE}
sensitivity <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TP]/(confusion[TP]+confusion[FN])
  
  return(value)
}

sensitivity(class.df)
```

###7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.
$$Specificity = \frac{TN}{TN+FP}$$
```{r, eval=FALSE, echo=TRUE}
specificity <- function(data, actual.idx=9, predict.idx=10){
  TP <- 4
  TN <- 1
  FP <- 3
  FN <- 2
  
  confusion <- table(data[,actual.idx], data[,predict.idx])
  value <- confusion[TN]/(confusion[TN]+confusion[FP])
  
  return(value)
}

specificity(class.df)
```

###8. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions.
$$F1 Score = \frac{2 X Precision X Sensitivity}{Precision + Sensitivity}$$
<<<<<<< HEAD
```{r, eval=FALSE,echo=TRUE}
=======
```{r, eval=FALSE, echo=TRUE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
f1Score <- function(data, actual.idx=9, predict.idx=10){
  confusion <- table(data[,actual.idx], data[,predict.idx])
  
  value <- (2*precision(data)*sensitivity(data))/(precision(data)+sensitivity(data))
  
  return(value)
}

f1Score(class.df)
```

###9. Before we move on, let's consider a question that was asked: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1. (Hint: If 0 < a < 1 and 0 < b < 1 then ab < a.)
```{r}
#F1 score reaches its best value at 1 and worst at 0.
#since both presition and sensitivity are in between 0 and 1
# we know 0 < p*s <=1 and 2*p*s <=p +s (maximum value p and s can have is 1 )
# so 2*p*s/p +s can have values in between 0 and 1

```
F1 score reaches its best value at 1 and worst at 0.
since both presition and sensitivity are in between 0 and 1
we know 0 < p*s <=1 and 2*p*s <=p +s (maximum value p and s can have is 1 )
 so 2*p*s/(p +s) can have values in between 0 and 1

###10. Write a function that generates an ROC curve from a data set with a true classification column (class in our example) and a probability column (scored.probability in our example). Your function should return a list that  includes  the  plot  of  the  ROC  curve  and  a  vector  that  contains  the  calculated  area  under  the  curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.

<<<<<<< HEAD
```{r,eval=FALSE,  echo=TRUE}
=======
```{r, eval=FALSE, echo=TRUE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
getRoCData <- function(data)
{
  rocResults <- data.frame(Threshold=c(), Sn=c(), Sp=c())
  
  thresholds <- seq(0, 1, by=0.01)
  for(threshold in thresholds)
  {
    data <- data %>% mutate(predicted = as.numeric(scored.probability >= threshold))
    data$predicted  <- factor( data$predicted , levels=c(0,1) )
    
    Sn <- sensitivity(data, actual.idx =  9, predict.idx =  12)
    Sp <- specificity(data, actual.idx =  9, predict.idx =  12)
    
    rocResults <- rbind(rocResults, data.frame(Threshold=c(threshold), Sn=c(Sn), Sp=c(Sp)))
  }
  
  auc <- cumsum(rocResults$Sn - (1-rocResults$Sp))*0.01
 
  rocPlot <- ggplot(rocResults, aes(x=1-Sp, y=Sn) ) + geom_line(size = 1, alpha = 0.6) +
    geom_point(data=rocResults[rocResults$Threshold == 0.5,], 
               aes(x=1-Sp, y=Sn), colour="orange") +
    geom_abline(slope=1, intercept=0, colour="blue", size=0.50) +
          xlim(0, 1) + ylim(0, 1) +
    labs(title="ROC Curve",
         x = "False Positive Rate ( 1 - Sp) ",
         y = "True Positive Rate (Sn)")

  results <- list(plot = rocPlot, auc = auc)

  return(results)
}

result <- getRoCData(class.df)
result$plot
result$auc
```

###11. Use your **created R function** and the provided classifcation output data set to produce all of the classification metrics discussed above.
<<<<<<< HEAD
```{r,eval=FALSE, echo=TRUE}
=======
```{r, eval=FALSE, echo=TRUE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
accuracy <- accuracy(class.df)
Sn <- sensitivity(class.df)
Sp <- specificity(class.df)
ClsErrRate <- classErrorRate(class.df)
f1 <- f1Score(class.df)
precsn <- precision(class.df)

metrics <- data.frame(Metric=c(), Value=c())
metrics <- rbind(metrics, data.frame(Metric = "Accuracy", Value = round(accuracy,4)))
metrics <- rbind(metrics, data.frame(Metric = "Sn(True Positive Rate)", Value = round(Sn,4)))
metrics <- rbind(metrics, data.frame(Metric = "Sp(True Negative Rate)", Value = round(Sp,4)))
metrics <- rbind(metrics, data.frame(Metric = "ClassErrorRate", Value = round(ClsErrRate,4)))
metrics <- rbind(metrics, data.frame(Metric = "F1Score", Value = round(f1,4)))
metrics <- rbind(metrics, data.frame(Metric = "Precision", Value = round(precsn,4)))

kable(metrics)
```

###12. Investigate the **caret** package. In particular, consider the functions confusionMatrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions?

**Confusion Matrix**
<<<<<<< HEAD
```{r,  eval=FALSE,echo=TRUE}
=======
```{r, eval=FALSE, echo=TRUE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
confusionMatrix(class.df$scored.class, class.df$class, positive = "1")
```

**Sensitivity**
<<<<<<< HEAD
```{r,  eval=FALSE,echo=TRUE}
=======
```{r, eval=FALSE, echo=TRUE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
round(caret::sensitivity(factor(class.df$scored.class), 
            factor(class.df$class), 
            positive="1"), 4)
```

**Specificity**
```{r,  eval=FALSE,echo=TRUE, warning=FALSE, message=FALSE}
round(caret::specificity(factor(class.df$scored.class), 
                   factor(class.df$class), 
                   negative="0"),4)
```
We compared the caret confusionmatrix with the one we created; all the measures are matched with the manually created one. Specificity and sensitivity are also same as we calculated.


**lift**
<<<<<<< HEAD
```{r,echo=TRUE}
=======
```{r, eval=FALSE, echo=TRUE}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
(liftResult <- lift(data=class.df, factor(class) ~ scored.probability))
graphics::plot(liftResult, plot = "gain", title(main="Sensitivity Vs Support"))
graphics::plot(liftResult, plot = "lift")
```

###13. Investate the **pROC** package. Use it to generate an ROC curve for the data set. How do the results compare with your own functions?

<<<<<<< HEAD
```{r,echo=TRUE, options(width = 80)}
=======
```{r, eval=FALSE, echo=TRUE, options(width = 80)}
>>>>>>> af9b1899e4764be884ddc57c28eaa90bd157aa15
(roc <- roc(factor(class)~scored.probability,data=,class.df, plot=FALSE, ci=TRUE))
graphics::plot(roc, legacy.axes = TRUE, col="blue", lwd=3)
auc(factor(class)~scored.probability,class.df)

```
The RoC curve is very similar to our RoC plot drawn using the getRoCData() function above.

